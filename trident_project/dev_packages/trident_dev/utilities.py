# AUTOGENERATED! DO NOT EDIT! File to edit: nbs/06_utilities.ipynb (unless otherwise specified).

__all__ = ['nostdout', 'path_insensitive', 'get_files', 'find_imagepath_from_stem', 'get_image_size',
           'write_list_to_file', 'sample_file', 'write_object_to_file', 'load_object_from_file', 'split_by_random',
           'write_train_validate_files', 'LabelFixer', 'convert_xml_labels_to_csv', 'count_classes', 'get_one_batch',
           'get_dataloader_item', 'save_tensor_to_csv', 'display_tensor', 'cv2_imshow', 'preds_per_cat',
           'falsepositives_per_cat', 'write_xml_for_fullimage', 'describe_image', 'fullsize_stats', 'PascalVocWriter',
           'PascalVocReader', 'reorder_paired_lists']

# Cell
import sys
from pathlib import Path
sys.path.insert(1,str(Path.cwd().parent))

# Cell
import cv2
import PIL
from PIL import Image
import numpy as np
import pandas as pd
import random
from torch import randperm

import contextlib
import io
import sys
import torch #,torchvision
import json
import jsonpickle
import xml.etree.ElementTree as etree

# Cell
@contextlib.contextmanager
def nostdout():
    """Suppress output from a function.  Usage:
    with nostdout():
        func()
    """
    save_stdout = sys.stdout
    sys.stdout = io.BytesIO()
    yield
    sys.stdout = save_stdout

# Cell
def path_insensitive(path):
    """
    Get a case-insensitive path for use on a case sensitive system.
    Given a path (file or directory) that matches an existing path on disk, the function will
    return the first path that matches it regardless of case.  If not found, returns the input.
    Parameters:
    path: str. A full path to a file or directory.
    Credit: Chris Morgan http://portableapps.hg.sourceforge.net/hgweb/portableapps/development-toolkit/file/775197d56e86/utils.py#l78.
    """
    return _path_insensitive(path) or path

# Cell
def get_files(fpath,extensions,recursive=False):
    """
    Get files with names whose suffixes match a list of extensions from a directory path.
    Returns a list of filenames, or an error if nothing is found.
    Parameter:
    fpath: str. Directory to search in
    extensions: list of strings.  Allowable file extensions.
    recursive: boolean.  Whether to search recursively (i.e., include subdirectories)
    """
    p = Path(fpath)
    if recursive:
        filelist = [str(x) for x in p.rglob('*')  if x.suffix in extensions]
    else:
        filelist = [str(x) for x in p.glob('*')  if x.suffix in extensions]
    assert len(filelist) > 0, f"No files matching extensions {extensions} found in {fpath}"
    return(filelist)

# Cell
def find_imagepath_from_stem(imagelist,filestem):
    """Searches for a filename that is only a stem (no path or extension) in a list of full filepaths.
        Returns
        - the full path if it finds exactly one match;
        - throws an error if >1 match;
        - returns -1 if not found.
    """
    fpath = [x for x in images if Path(x).stem==filestem]
    if len(fpath) > 0:
        assert len(fpath)==1, f"More than one filename matches the stem {fname}"
        filepath = fpath[0]
        return filepath
    else:
        print("Image not found:",filestem)
        return -1

# Cell
def get_image_size(filepath):
    with Image.open(filepath) as pil_img:
        (width,height) = pil_img.size
    return (width,height)

# Cell
def write_list_to_file(l,filepath):
    """Write list l to a plain text file, one item per line."""
    with open (filepath,'w') as f:
        for item in l:
            f.write("%s\n" % item)
    f.close()

def sample_file(filename, nrows):
    """Take a sample of rows (nrows long) from a csv file.  Returns the first column only, as a list.  Assumes no header"""
    samp = pd.read_csv(filename, header = None).sample(nrows)
    return list(samp[0])

def write_object_to_file(obj,outpath,verbose=True):
    """Encode object using jsonpickle.encode and write to outpath"""
    with open(outpath, 'w') as json_file:
        enc_results = jsonpickle.encode(obj)
        json.dump(enc_results, json_file)
        if verbose:
            print(obj,'written to',outpath)

def load_object_from_file(fpath):
    """Load an object from a file using jsonpickle.decode.
        REMINDER: the original object class must be accessible when decoding, to decode properly."""
    if Path(fpath).is_file():
        with open(fpath, 'r') as f:
            try:
                data_encoded = json.load(f)
                data = jsonpickle.decode(data_encoded)
            except (json.decoder.JSONDecodeError, AttributeError):
                print('jsonpickle unable to decode file',fpath)
                raise
    elif Path(fpath).is_dir():
        raise ValueError('{} must be a file, not a directory'.format(fpath))
    else:
        raise ValueError('{} not found'.format(fpath))
    return data

# Cell
def split_by_random(o,valid_pct):
    """Split filenames into train and validate sets (copied from fastai2)"""
    rand_idx = [int(i) for i in randperm(len(o))]
    cut = int(valid_pct * len(o))
    return rand_idx[cut:],rand_idx[:cut]

# Cell
def write_train_validate_files(rootdir,annotation_files,train_idxs,val_idxs):
    """Writes two text files to the root directory that define training and validation data sets.
       They contain filenames without path or suffix.
       """
    trfile = open(str(rootdir/'train.txt'),'w')
    for idx in train_idxs:
        fname = Path(annotation_files[idx]).stem
        trfile.write(fname)
        trfile.write('\n')
    trfile.close()

    valfile = open(str(rootdir/'valid.txt'),'w')
    for idx in val_idxs:
        fname = Path(annotation_files[idx]).stem
        valfile.write(fname)
        valfile.write('\n')
    valfile.close()

# Cell
class LabelFixer:
    """Returns the original if the label is in the permitted_labels list,
    a renamed label if it is in the rename_dict, 'None' if the label is in the
    omit_labels list, or "other_animal" if it is not in the any of the above.

    Parameters:
    permitted_labels (list of strings): labels to keep as-is
    omit_labels (list of strings): labels to drop
    rename_dict (dict, oldname:newname): labels to rename
    """
    def __init__(self, permitted_labels, omit_labels, rename_dict):
        self.permitted_labels = permitted_labels
        self.omit_labels = omit_labels
        self.rename_dict = rename_dict #(oldname:newname)

    def fix(self, label):
        if label in omit_labels:
            return None
        else:
            fixed = self._maybe_rename_category(label)
            if not fixed in self.permitted_labels:
                fixed = "other_animal"
        return fixed

    def _maybe_rename_category(self, category):
        """Rename a category if the category key appears in the rename_dict, which contains
           { oldname : newname }.  Returns the original category (i.e., no-op)
           if it is not found in the dict keys.
        """
        if category in self.rename_dict:
            newcategory = self.rename_dict[category]
        else:
            newcategory = category
        return newcategory

# Cell
def convert_xml_labels_to_csv(xml_files,label_fixer):
    """Extracts object labels from Pascal-VOC style XML annotation files (one file per image),
    fixes the label names as needed, and returns a dataframe with two columns: filename and labels,
    where labels is a space-delimited list.
    This function was made for loading annotations from XML files to a fastai multiclassification model.
    """
    parser = et.XMLParser(remove_blank_text=True)
    sep = ' '
    data = [] #a list of tuples
    for i in range(len(xml_files)):
        tree = et.parse(str(xml_files[i]),parser)
        fname = tree.xpath('//filename')[0].text + '.jpg'#should only be one per file
        names = []
        for obj in tree.findall("object"):
            category = obj.find("name").text
            #1. Skip over instances if the instance class is in the 'omit_classes' list,
            #2. Rename those in 'rename_classes' dict, and
            #3. Rename any other classes not recognized to "other_animal"
            category = label_fixer.fix(category)
            if not category is None:
                names.append(category)
        names = set(names)
        if len(names)==0:
            labels = '' #to avoid getting a NaN
        else:
            labels = sep.join(names) #join names in a space-separated string
        tup = (fname,labels)
        data.append(tup)
    df = pd.DataFrame(data, columns =['filename', 'labels'])
    return df

# Cell
def count_classes(annotation_files,count_empty=True):
    """Count object classes in xml annotation files.  By default, will add a count of 'empty' images
    that contain no objects.  Note that there could be multiple instances per image.
    Parameter:
    annotation_files: list of strings or Pathlib paths.
    """
    counts = {'empty_image':0}
    for f in annotation_files:
        tree = etree.parse(str(f))
        objects = tree.findall("object")
        if ((count_empty) and (len(objects)==0)):
            counts['empty_image'] += 1
        else:
            for obj in tree.findall("object"):
                cls = obj.find("name").text
                if cls in counts:
                    counts[cls] += 1
                else:
                    counts[cls] = 1
    return(list(counts.items()))

# Cell
def get_one_batch(dataloader):
    """Returns one batch from a dataloader"""
    iter_dl = iter(dataloader) #Necessary.  You have to tell the fucking thing it's iterable.  Why?
    batch = next(iter_dl)
    return(batch)

def get_dataloader_item(dataloader,indx):
    """Examine a particular input from a dataloader
    """
    etl = enumerate(dataloader)
    found = False
    for idx, inputs in etl:
        if idx==indx:
            return inputs
    if found == False:
        return "Index not found"

# Cell
def save_tensor_to_csv(t,outfile,rownames=None,colnames=None):
    """Convenience function: converts a 1- or 2D tensor to a numpy array, then to a Pandas dataframe,
    then saves it as a csv file."""
    if len(t.shape) > 2:
        print("Tensor not saved: dimension > 2.")
        return
    else:
        t_np = t.numpy()
        df = pd.DataFrame(t_np)
        if (colnames is not None) and (len(colnames)==t.shape[1]):
            df.columns = colnames
        if (rownames is not None) and (len(rownames)==t.shape[0]):
            df.index = rownames
            df.to_csv(outfile)
        else:
            df.to_csv(outfile,index=False)
        print("Tensor saved as Pandas dataframe to",outfile)
        return df

# Cell
def display_tensor(pttensor):
    """Convert a Pytorch tensor to a Numpy array and display with cv2.
    """
    nparray = pttensor.permute(1,2,0).numpy()
    cv2_nparray = nparray[:,:,::-1] #switch to BGR for cv2
    cv2_imshow(cv2_nparray)

def cv2_imshow(a):
    """A replacement for cv2.imshow() for use in Jupyter notebooks.
    Args:
    a : numpy.ndarray; color channels assumed to be in RGB order
    Note if:
        shape = (N, M) or (N, M, 1) then 'a' is an NxM grayscale image.
        shape = (N, M, 3) then 'a' is an NxM BGR color image.
        shape (N, M, 4) then 'a' is an NxM BGRA color image.
    """
    a = a.clip(0, 255).astype('uint8')
    # cv2 stores colors as BGR; convert to RGB
    if a.ndim == 3:
        if a.shape[2] == 4:
            a = cv2.cvtColor(a, cv2.COLOR_BGRA2RGBA)
        else:
            a = cv2.cvtColor(a, cv2.COLOR_BGR2RGB)
    display(Image.fromarray(a))

# Cell
def preds_per_cat(targs,bin_preds,cat):
    """ Compares model predictions with ground-truth labels for multi-label classification.
    Returns a column-wise sum of predictions by category, for each true category.
    In the output table, the true category is the row and the predictions made are the columns.

    targs: targets (tensor, n_images x n_categories), i.e., ground-truth labels
    bin_preds: binary predictions (tensor, same shape as targets)
    cat: the category index (for example, a number between 0-18 if there are 19 categories)
    """
    cat_idx = (targs[:,cat] > 0) #only rows where the category was seen
    dist = torch.sum(bin_preds[cat_idx],dim=0)
    return dist

# Cell
def falsepositives_per_cat(targets,bin_predictions,category):
    """ Compares model predictions with ground-truth labels for multi-label classification.
    Returns a table that shows the number of false-positives for each category,
    expressed as the sum of the rows for which pred=1 and target=0, taken from
    the subset of rows where pred=0 and target=1.  In other words, it answers the
    question: 'In an image where the model was wrong about the target category, what other categories
    were false-positives?'  The output rows are target(=ground-truth) categories and
    the columns are prediction categories in the same order.

    targs: targets (tensor, n_images x n_categories), i.e., ground-truth labels
    bin_preds: binary predictions (tensor, same shape as targets)
    category: the category index (for example, a number between 0-18 if there are 19 categories)
    """
    #Explanation:
    #(targs[:,category] > 0) is rows where the focal category was seen (Y=1)
    #(bin_preds[:,category]==0) is rows where the focal category was not predicted
    missed_idx = (targs[:,category] > 0) & (bin_preds[:,category]==0) #Rows where the focal category was missed (i.e., target=1 and prediction=0)

    #This line is tricky; it finds non-focal categories where the target=0 and the prediction=1 in the subset of missed rows.
    #We're comparing matrices of size [missed_rows,categories] so the resulting index is also of size [missed_rows,categories].
    false_pos_idx = (targs[missed_idx]==0) & (bin_preds[missed_idx]>0)
    dist = torch.sum(false_pos_idx,dim=0)
    return dist

# Cell
def write_xml_for_fullimage(df,fname,width,height,tile_size,overlap,outdir):
    """Writes a Pascal-VOC-style XML annotation file for _ONE_ full-sized image.
        df: a Pandas dataframe in the bpred style of binary predictions from a classification model ,
        with one row per tile.
        fname: filename for a full-sized image
        tile_size: tuple (height,width) in pixels
        overlap: int.  Tile overlap in pixels (assuming vertical and horizontal overlap are equal)
    """

    writer = PascalVocWriter(fname, width, height) # Writer(image_filepath, image_width, image_height)

    non_empty_mask = df[(df['filename']==Path(fname).stem) & (df['empty']==0)].index
    ntiles = len(non_empty_mask)
    if ntiles==0:
#        print("No objects in file",fname)
        return
    #A non-empty row corresponds to a tile that contains an object; we make that tile into a bbox
    nonemptyrows = df.loc[non_empty_mask]
    for i in range(len(nonemptyrows)):
        cn = nonemptyrows.iloc[i].index #index of the row = the column names.
        cv = nonemptyrows.iloc[i].values #values in this row
        tilerow = cv[1]
        tilecol = cv[2]
        sumvals = sum(cv[3:])
        #If none of the categories are 1 including the 'empty' row, then the model is uncertain
        obj_str = ''
        if sumvals == 0:
            obj_str = 'uncertain'
        #else we compile a string of whatever is in each tile
        else:
            for j in range(4,len(cn)):
                if cv[j] > 0:
                    obj_str += cn[j] + ' '

        score = -1 #
        xmin,ymin = get_tile_offsets(tilerow,tilecol,tile_size,overlap)
        xmax = xmin + tile_size[1]
        ymax = ymin + tile_size[0]
        writer.addObject(obj_str, xmin, ymin, xmax, ymax, score=score, pose='Unspecified', truncated=0, difficult=0)

    outfile = str((Path(outdir)/fname).with_suffix('.xml'))
    writer.save(outfile)
    ntiles += 1
    return ntiles

# Cell
def describe_image(pred_df,fname):
    """A convenience function; writes a string that describes the tiles in a full-sized image.
        It is designed to work with a prediction dataframe that has the following columns:
        filename, tile_row, tile_col, empty, category_1, category_2...category_n, where
        the filename is the original full-size image filename, stem only (no extension or path)
        and the values are 1/0 for each category.
    """
    ostring = '' #the description string we will build
    tiles_mask = pred_df[pred_df['filename']==fname].index
    tilerows = pred_df.loc[tiles_mask]
    ntiles = len(tilerows)
    nempty = sum(pred_df.loc[tiles_mask]['empty'])
    if(ntiles == nempty):
        ostring = 'empty'
    else:
        non_empty_mask = pred_df[(pred_df['filename']==fname) & (pred_df['empty']==0)].index
        nonemptyrows = pred_df.loc[non_empty_mask]
        for i in range(len(nonemptyrows)):
            cn = nonemptyrows.iloc[i].index #index of the row = the column names.
            cv = nonemptyrows.iloc[i].values #values in this row
            rowcol = '[' + str(cv[1]) + ']' + '[' + str(cv[2]) + ']' #row, col written as '[0][5]' etc.
            sumvals = sum(cv[3:])
            obj_str = rowcol + ':'
            #If none of the categories are 1 including the 'empty' row, then the model is uncertain
            if sumvals == 0:
                obj_str += 'uncertain'
            #else we compile a string of whatever is in each tile
            else:
                for j in range(4,len(cn)):
                    if cv[j] > 0:
                        obj_str += cn[j] + ' '
            if len(obj_str) > 0:
                ostring += ' ' + obj_str
    return ostring


# Cell
def fullsize_stats(f,bpred):
    """Summarizes a binary prediction file that has one row per tile.
    Calculates four items for a full-size file:
    1. is_empty (all tiles are empty)
    2. has_objects
    2. has_elephants (at least one tile has an elephant)
    3. is_uncertain; i.e. the file is a mix of empty and uncertain tiles but contains no other objects.
    """
    fmask = bpred[(bpred['filename']==f)].index
    tilerows = bpred.loc[fmask]
    n_tiles = len(tilerows)
    n_empty = len(tilerows[tilerows['empty']==1])
    n_elephants = len(tilerows[tilerows['elephant']==1])
    non_empty_mask = tilerows[tilerows['empty']==0].index
    nonemptyrows = tilerows.loc[non_empty_mask]
    n_uncertain = sum((nonemptyrows.iloc[:,3:].apply(sum,axis=1).values) == 0)
    n_objectrows = sum(nonemptyrows.iloc[:,4:].apply(sum,axis=1).values)
    is_empty = (True if n_tiles == n_empty else False)
    has_elephants = (True if n_elephants > 0 else False)
    is_uncertain = (True if (n_tiles - n_empty == n_uncertain) & (n_uncertain > 0) else False)
    has_objects = (True if n_objectrows > 0 else False)
    return (is_empty,has_objects,has_elephants,is_uncertain)

# Cell
#Adapted from https://github.com/tzutalin/labelImg/blob/master/libs/pascal_voc_io.py
# -*- coding: utf8 -*-

class PascalVocWriter:
    """
    Write a PascalVOC-formatted XML annotation file.  This is designed as a drop-in replacement for
    pascal_voc_writer.  Two steps: 1) create the file via __init__ and 2) add objects, one by one.
    """
    def __init__(self, path, width, height, depth=3, database='Unknown', segmented=0):
        self.ENCODE_METHOD = 'utf-8'
        self.XML_EXT = '.xml'
        self.path = str(path)
        self.foldername = str(Path(path).parent)
        self.filename = str(Path(path).name)
        self.localImgPath = None
        self.databaseSrc = database
        self.width = width
        self.height = height
        self.depth = depth
        self.segmented = segmented
        self.verified = False
        self.root = self.genXML()

    def prettify(self, elem):
        """
            Return a pretty-printed XML string for the Element.
        """
        rough_string = ElementTree.tostring(elem, 'utf8')
        root = etree.fromstring(rough_string)
        return etree.tostring(root, pretty_print=True, encoding=self.ENCODE_METHOD).replace("  ".encode(), "\t".encode())

    def genXML(self):
        """
            Return XML root
        """
        # Check conditions
        if self.filename is None or \
                self.foldername is None or \
                self.height is None or \
                self.width is None:
            print("Missing filename, foldername, height or width in genXML")
            return None

        root = Element('annotation')
        if self.verified:
            root.set('verified', 'yes')

        folder = SubElement(root, 'path')
        folder.text = self.path

        folder = SubElement(root, 'folder')
        folder.text = self.foldername

        filename = SubElement(root, 'filename')
        filename.text = self.filename

        if self.localImgPath is not None:
            localImgPath = SubElement(root, 'path')
            localImgPath.text = self.localImgPath

        source = SubElement(root, 'source')
        database = SubElement(source, 'database')
        database.text = self.databaseSrc

        size_part = SubElement(root, 'size')
        width = SubElement(size_part, 'width')
        height = SubElement(size_part, 'height')
        depth = SubElement(size_part, 'depth')
        width.text = str(self.width)
        height.text = str(self.height)
        if self.depth == 3:
            depth.text = str(self.depth)
        else:
            depth.text = '1'

        segmented = SubElement(root, 'segmented')
        segmented.text = str(self.segmented)
        return root

    #Called once per object
    def addObject(self, name, xmin, ymin, xmax, ymax, score=-1, pose='Unspecified', truncated=0, difficult=0,attributes=[]):
        """Attributes that are not individually named in the function signature can be passed as a list of dicts,
        where each dict has the form {'name':<attribute_name>, 'value':<attribute_value>}
        """
        object_item = SubElement(self.root, 'object')
        oclass = SubElement(object_item, 'name')
        oclass.text = str(name) #ustr(name)
        pose = SubElement(object_item, 'pose')
        pose.text = "Unspecified"
        obj_score = SubElement(object_item, 'score')
        obj_score.text = str(score)
        #attributes= a list of dicts; each dict is one attribute and looks like: {'name':<name>, 'value':<value>}
        for i in range(len(attributes)):
            atr_name = attributes[i]['name']
            atr_value = str(attributes[i]['value'])
            oatr = SubElement(object_item,atr_name)
            oatr.text = atr_value

        bbox = {'xmin': xmin, 'ymin': ymin, 'xmax': xmax, 'ymax': ymax}
        bndbox = SubElement(object_item, 'bndbox')
        xmin = SubElement(bndbox, 'xmin')
        xmin.text = str(bbox['xmin'])
        ymin = SubElement(bndbox, 'ymin')
        ymin.text = str(bbox['ymin'])
        xmax = SubElement(bndbox, 'xmax')
        xmax.text = str(bbox['xmax'])
        ymax = SubElement(bndbox, 'ymax')
        ymax.text = str(bbox['ymax'])

#         #Add a boolean (truncated) to indicate if width or height were truncated (works, but unnecessary)
#         truncated = SubElement(object_item, 'truncated')
#         if int(float(bbox['ymax'])) == int(float(self.height)) or (int(float(bbox['ymin']))== 1):
#             truncated.text = "1" # max == height or min
#         elif (int(float(bbox['xmax']))==int(float(self.width))) or (int(float(bbox['xmin']))== 1):
#             truncated.text = "1" # max == width or min
#         else:
#             truncated.text = "0"
        difficult = SubElement(object_item, 'difficult')
        difficult.text = str( bool(difficult) & 1 )

    #Full filepath
    def changePath(self,newpath):
        root = self.root
        root.findall('path')[0].text = newpath

    #Just the filename, including extension
    def changeFilename(self,newfilename):
        root = self.root
        root.findall('filename')[0].text = newfilename

    #The parent folder of the file
    def changeFolder(self,newfolder):
        root = self.root
        root.findall('folder')[0].text = newfolder

    def save(self, targetFile=None):
        root = self.root
        out_file = None
        if targetFile is None:
            out_file = codecs.open(
                self.filename + self.XML_EXT, 'w', encoding=self.ENCODE_METHOD)
        else:
            out_file = codecs.open(targetFile, 'w', encoding=self.ENCODE_METHOD)

        prettifyResult = self.prettify(root)
        out_file.write(prettifyResult.decode('utf8'))
        out_file.close()


class PascalVocReader:
    """"I haven't tested this function"""
    def __init__(self, filepath):
        # shapes type:
        # [labbel, [(x1,y1), (x2,y2), (x3,y3), (x4,y4)], color, color, difficult]
        self.shapes = []
        self.filepath = filepath
        self.verified = False
        try:
            self.parseXML()
        except:
            pass

    def getShapes(self):
        return self.shapes

    def addShape(self, label, bndbox, difficult):
        xmin = int(float(bndbox.find('xmin').text))
        ymin = int(float(bndbox.find('ymin').text))
        xmax = int(float(bndbox.find('xmax').text))
        ymax = int(float(bndbox.find('ymax').text))
        points = [(xmin, ymin), (xmax, ymin), (xmax, ymax), (xmin, ymax)]
        self.shapes.append((label, points, None, None, difficult))

    def parseXML(self):
        assert self.filepath.endswith(self.XML_EXT), "Unsupported file format"
        parser = etree.XMLParser(encoding=self.ENCODE_METHOD)
        xmltree = ElementTree.parse(self.filepath, parser=parser).getroot()
        filename = xmltree.find('filename').text
        try:
            verified = xmltree.attrib['verified']
            if verified == 'yes':
                self.verified = True
        except KeyError:
            self.verified = False

        for object_iter in xmltree.findall('object'):
            bndbox = object_iter.find("bndbox")
            label = object_iter.find('name').text
            # Add chris
            difficult = False
            if object_iter.find('difficult') is not None:
                difficult = bool(int(object_iter.find('difficult').text))
            self.addShape(label, bndbox, difficult)
        return True

# Cell
def reorder_paired_lists(list1,key1,list2):
    """Takes a list of dicts (list1) with a key field that should match the order of list2,
    and returns a reordered version of list2 so the orders match."""
    assert len(list1)==len(list2),"Lists are different lengths."
    idxs = [list2.index(el1[key1]) for el1 in list1]
    newl2 = [list2[i] for i in idxs]
    return newl2
# a = [{'name':'john'},{'name':'ann'},{'name':'liz'}]
# b = ['liz','john','ann']
# reorder_paired_lists(a,'name',b) #output: ['john','ann','liz']