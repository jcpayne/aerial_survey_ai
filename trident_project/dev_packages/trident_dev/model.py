# AUTOGENERATED! DO NOT EDIT! File to edit: nbs/02_custom_model.ipynb (unless otherwise specified).

__all__ = ['build_augmentation', 'RandomCorruption', 'TridentDatasetMapper', 'TileDatasetMapper', 'IterableTileDataset',
           'build_train_loader', 'JTFMEvaluator', 'Trainer', 'JP_RepeatFactorTrainingSampler']

# Cell
import math
import random
import itertools
import numpy as np
import torch
from torch import nn
from torch.utils.data.distributed import DistributedSampler
from torchvision import transforms as transforms
import imgaug as ia
from imgaug import augmenters as iaa
import imagecorruptions
from adabound import AdaBound
from typing import List, Optional, Union
from collections import defaultdict
import math
import logging

#Detectron imports
import detectron2
from detectron2.config import configurable
from detectron2.data import transforms as T
from detectron2.data.transforms import TransformGen
from detectron2.data import DatasetCatalog, MetadataCatalog, DatasetMapper,build_detection_test_loader
from detectron2.data import DatasetFromList, MapDataset, detection_utils as utils, get_detection_dataset_dicts
from detectron2.data import transforms as T
from detectron2.data.samplers import InferenceSampler, RepeatFactorTrainingSampler
from detectron2.data.build import build_detection_test_loader, build_batch_data_loader, trivial_batch_collator
from detectron2.data.transforms import TransformGen
from fvcore.transforms.transform import (BlendTransform,NoOpTransform,Transform)
from detectron2.engine import DefaultTrainer
from detectron2.evaluation import DatasetEvaluator,DatasetEvaluators,COCOEvaluator
from detectron2.solver.build import *
#from detectron2.solver import build_optimizer
import detectron2.utils.comm as comm

# Cell
def build_augmentation(cfg, is_train):
    """Overrides the default method (in detection_utils.py) to add image corruption as an augmentation."""
    #logger = logging.getLogger(__name__)

    result = utils.build_augmentation(cfg, is_train) #Returns list[Augmentation(s)] containing default behavior

    if is_train:
        random_corruption = RandomCorruption(cfg.INPUT.corruption) #prob of adding corruption ([0,1])
        result.append(random_corruption)
        print("Random corruption augmentation used in training")

        #logger.info("Random corruption augmentation used in training: " + str(random_corruption))
        print(result)
    return result



# Cell
class RandomCorruption(TransformGen):
    """
    Randomly transforms image corruption using the 'imgaug' package
    (which is only guaranteed to work for uint8 images).
    Returns an Numpy ndarray.
    """

    def __init__(self, p):
        """
        Args:
            p probability of applying corruption (p is on [0,1])
        """
        super().__init__()
        self._init(locals())
        self.p = p

    def get_transform(self, img):
        r = random.random()
        if(r <= self.p):
            #A selection of effects from imgaug
            #ia.seed(None)
            severity = random.randint(1,5)
            augmenter_list = [
                iaa.BlendAlphaSimplexNoise(
                    foreground=iaa.EdgeDetect(alpha=(0.5, 0.9)),
                    background=iaa.LinearContrast((0.5, 0.2)),
                    per_channel=0.5),
                iaa.CoarseDropout(p=0.25, size_px=8),
                iaa.imgcorruptlike.GaussianBlur(severity),
                iaa.imgcorruptlike.SpeckleNoise(severity),
                iaa.Cutout(fill_mode="gaussian", fill_per_channel=True,nb_iterations=(1, 5), size=0.2, squared=False),
                iaa.imgcorruptlike.Spatter(severity)]
            #Blend noise with the source image
            augmenter = random.choice(augmenter_list)
            blended_img = augmenter.augment_image(img)
            return BlendTransform(src_image=blended_img, src_weight=1, dst_weight=0)
        else:
            return(NoOpTransform())

# Cell
class TridentDatasetMapper(DatasetMapper):
    """
    A customized version of DatasetMapper.
    A callable which takes a dataset dict in Detectron2 Dataset format,
    and maps it into a format used by the model.

    """
    #The only change I made is to switch build_augmentation for utils.build_augmentation
    @classmethod
    def from_config(cls, cfg, is_train: bool = True):
        augs = build_augmentation(cfg, is_train) #changed here
        if cfg.INPUT.CROP.ENABLED and is_train:
            augs.insert(0, T.RandomCrop(cfg.INPUT.CROP.TYPE, cfg.INPUT.CROP.SIZE))
            recompute_boxes = cfg.MODEL.MASK_ON
        else:
            recompute_boxes = False

        ret = {
            "is_train": is_train,
            "augmentations": augs,
            "image_format": cfg.INPUT.FORMAT,
            "use_instance_mask": cfg.MODEL.MASK_ON,
            "instance_mask_format": cfg.INPUT.MASK_FORMAT,
            "use_keypoint": cfg.MODEL.KEYPOINT_ON,
            "recompute_boxes": recompute_boxes,
        }
        if cfg.MODEL.KEYPOINT_ON:
            ret["keypoint_hflip_indices"] = utils.create_keypoint_hflip_indices(cfg.DATASETS.TRAIN)

        if cfg.MODEL.LOAD_PROPOSALS:
            ret["precomputed_proposal_topk"] = (
                cfg.DATASETS.PRECOMPUTED_PROPOSAL_TOPK_TRAIN
                if is_train
                else cfg.DATASETS.PRECOMPUTED_PROPOSAL_TOPK_TEST
            )
        return ret

# Cell
class TileDatasetMapper(DatasetMapper):
    """
    Just like DatasetMapper except instead of opening the file, it is passed a dict
    containing an already-opened file.
    Note: the __init__ class is decorated with @configurable so you can pass
    a cfgNode object and it will use the from_config() method for initiation.

    1. Accept an opened image as a Pytorch array
    2. Potentially applies cropping/geometric transforms to the image and annotations
    """
    @configurable
    def __init__(
        self,
        is_train: bool,
        *,
        augmentations: List[Union[T.Augmentation, T.Transform]],
        image_format: str,
        use_instance_mask: bool = False,
        use_keypoint: bool = False,
        instance_mask_format: str = "polygon",
        keypoint_hflip_indices: Optional[np.ndarray] = None,
        precomputed_proposal_topk: Optional[int] = None,
        recompute_boxes: bool = False
    ):
        if recompute_boxes:
            assert use_instance_mask, "recompute_boxes requires instance masks"
        # fmt: off
        self.is_train               = is_train
        self.augmentations          = augmentations
        self.image_format           = image_format
        self.use_instance_mask      = use_instance_mask
        self.instance_mask_format   = instance_mask_format
        self.use_keypoint           = use_keypoint
        self.keypoint_hflip_indices = keypoint_hflip_indices
        self.proposal_topk          = precomputed_proposal_topk
        self.recompute_boxes        = recompute_boxes
        # fmt: on
        logger = logging.getLogger(__name__)
        logger.info("Augmentations used in training: " + str(augmentations))

    @classmethod
    def from_config(cls, cfg, is_train: bool = False):
        augs = utils.build_augmentation(cfg, is_train) #returns T.ResizeShortestEdge, plus optionally others
        if cfg.INPUT.CROP.ENABLED and is_train:
            augs.insert(0, T.RandomCrop(cfg.INPUT.CROP.TYPE, cfg.INPUT.CROP.SIZE))
            recompute_boxes = cfg.MODEL.MASK_ON
        else:
            recompute_boxes = False

        ret = {
            "is_train": is_train,
            "augmentations": augs,
            "image_format": cfg.INPUT.FORMAT,
            "use_instance_mask": cfg.MODEL.MASK_ON,
            "instance_mask_format": cfg.INPUT.MASK_FORMAT,
            "use_keypoint": cfg.MODEL.KEYPOINT_ON,
            "recompute_boxes": recompute_boxes,
        }
        return ret

    def __call__(self, dataset_dict):
        """
        Args:
            dataset_dict (dict): Metadata of one image, in Detectron2 Dataset format.

        Returns:
            dict: a format that builtin models in detectron2 accept
        """
#         dataset_dict = copy.deepcopy(dataset_dict)  # it will be modified by code below
#         # USER: Write your own image loading if it's not from a file
#         # We just use the image that is passed in, instead of opening the file and converting
#         #The returned dict has 2 things in it: file_name and image (which contains a tensor).  That's all.
#         image = dataset_dict["image"]

#         image_shape = image.shape[1:]  # h, w

        if not self.is_train:
            return dataset_dict

# Cell
class IterableTileDataset(torch.utils.data.IterableDataset):
    """
    An IterableDataset for passing arbitrary-length data to a Dataloader. The
    iter() method ensures that data is not duplicated if there is more than one worker, by splitting
    the data into separate pieces and only passing each worker one piece to iterate.
    """
    def __init__(self, tilelist):
        super(IterableTileDataset).__init__() #The init method of IterableDataset
        self.data = tilelist
        self.start = 0
        self.end = len(self.data)

    def reset_dataset(self, newdataset):
        self.data = newdataset
        self.start = 0
        self.end = len(newdataset)

    def __iter__(self):
        worker_info = torch.utils.data.get_worker_info()
        print("IterableDataset worker_info.id",worker_info.id)
        if worker_info is None:  # single-process data loading, return the full iterator
            indexes = range(self.start,self.end)
        else:  # in a worker process
            # split workload
            per_worker = int(math.ceil((self.end - self.start) / float(worker_info.num_workers)))
            worker_id = worker_info.id
#            print()
            iter_start = self.start + worker_id * per_worker
            iter_end = min(iter_start + per_worker, self.end)
            indexes = range(iter_start, iter_end)
        itemlist = [self.data[t] for t in indexes]
        #print("worker_id:",worker_id,"items:",itemlist) #very useful for debugging
        return iter(itemlist)
    def __len__(self):
        return len(self.data)
    def __getitem__(self,idx):
        return self.data[idx]


# Cell
# class IterableTileDataset(torch.utils.data.IterableDataset):
#     """
#     An IterableDataset for passing arbitrary-length data to a Dataloader. The
#     iter() method ensures that data is not duplicated if there is more than one worker, by splitting
#     the data into separate pieces and only passing each worker one piece to iterate.
#     """
#     def __init__(self, tilelist):
#         super(IterableTileDataset).__init__() #The init method of IterableDataset
#         self.data = tilelist
#         self.start = 0
#         self.end = len(self.data)

#     def reset_dataset(self, newdataset):
#         self.data = newdataset
#         self.start = 0
#         self.end = len(newdataset)

#     def __iter__(self):
#         indexes = range(self.start,self.end)
#         itemlist = [self.data[t] for t in indexes]
#         return iter(itemlist)
#         #return iter(range(self.start, self.end))

#     def __len__(self):
#         return len(self.data)

#     def __getitem__(self,idx):
#         return self.data[idx]

# #NOTE: This function must live outside the dataset class (not here!)
# def worker_init_fn(worker_id):
#     worker_info = torch.utils.data.get_worker_info()
#     print("worker_info.id",worker_info.id,"worker_info.num_workers",worker_info.num_workers)
#     dataset = worker_info.dataset  # the dataset copy in this worker process
#     overall_start = dataset.start
#     overall_end = dataset.end
#     # configure the dataset to only process the split workload
#     per_worker = int(math.ceil((overall_end - overall_start) / float(worker_info.num_workers)))
#     worker_id = worker_info.id
#     dataset.start = overall_start + worker_id * per_worker
#     dataset.end = min(dataset.start + per_worker, overall_end)



# Cell
def build_train_loader(cfg, mapper=None):
    """
    This is an edited version of build_detection_train_loader; it adds JP_RepeatFactorTrainingSampler,
    which fixes a bug in RepeatFactorTrainingSampler that caused it to crash when it encountered empty images.

    A data loader is created by the following steps:

    1. Use the dataset names in config to query :class:`DatasetCatalog`, and obtain a list of dicts.
    2. Coordinate a random shuffle order shared among all processes (all GPUs)
    3. Each process spawn another few workers to process the dicts. Each worker will:
       * Map each metadata dict into another format to be consumed by the model.
       * Batch them by simply putting dicts into a list.

    The batched ``list[mapped_dict]`` is what this dataloader will yield.

    Args:
        cfg (CfgNode): the config
        mapper (callable): a callable which takes a sample (dict) from dataset and
            returns the format to be consumed by the model.
            By default it will be ``DatasetMapper(cfg, True)``.

    Returns:
        an infinite iterator of training data
    """
    dataset_dicts = get_detection_dataset_dicts(
        cfg.DATASETS.TRAIN,
        filter_empty=cfg.DATALOADER.FILTER_EMPTY_ANNOTATIONS,
        min_keypoints=cfg.MODEL.ROI_KEYPOINT_HEAD.MIN_KEYPOINTS_PER_IMAGE
        if cfg.MODEL.KEYPOINT_ON
        else 0,
        proposal_files=cfg.DATASETS.PROPOSAL_FILES_TRAIN if cfg.MODEL.LOAD_PROPOSALS else None,
    )
    dataset = DatasetFromList(dataset_dicts, copy=False)

    if mapper is None:
        mapper = DatasetMapper(cfg, True)
    dataset = MapDataset(dataset, mapper)

    sampler_name = cfg.DATALOADER.SAMPLER_TRAIN
    logger = logging.getLogger(__name__)
    logger.info("Using training sampler {}".format(sampler_name))
    # TODO avoid if-else?
    if sampler_name == "TrainingSampler":
        sampler = TrainingSampler(len(dataset))
    elif sampler_name == "RepeatFactorTrainingSampler":
        repeat_factors = RepeatFactorTrainingSampler.repeat_factors_from_category_frequency(
            dataset_dicts, cfg.DATALOADER.REPEAT_THRESHOLD
        )
        sampler = RepeatFactorTrainingSampler(repeat_factors)
    elif sampler_name == "JP_RepeatFactorTrainingSampler":
        repeat_factors = JP_RepeatFactorTrainingSampler.repeat_factors_from_category_frequency(
            dataset_dicts, cfg.DATALOADER.REPEAT_THRESHOLD
        )
        sampler = RepeatFactorTrainingSampler(repeat_factors)
    else:
        raise ValueError("Unknown training sampler: {}".format(sampler_name))
    return build_batch_data_loader(
        dataset,
        sampler,
        cfg.SOLVER.IMS_PER_BATCH,
        aspect_ratio_grouping=cfg.DATALOADER.ASPECT_RATIO_GROUPING,
        num_workers=cfg.DATALOADER.NUM_WORKERS,
    )


# Cell
class JTFMEvaluator(DatasetEvaluator):
    """
    A dead-simple, Just The Facts, Ma'am evaluator.  Just compiles and returns results.
    The `process()` method evaluates each iteration; the `evaluate()` method summarizes
    at the end.
    """

    def __init__(self,  distributed, output_dir=None):
        """
        Arguments:
            distributed (T/F): if True, will collect results from all ranks and run evaluation
                in the main process. Otherwise, will evaluate the results in the current process.
            output_dir (str): optional, an output directory to dump all
                results predicted on the dataset [not implemented at present]:
        """
        self._tasks = ("bbox",)
        self._distributed = distributed
        self._output_dir = output_dir

        self._cpu_device = torch.device("cpu")
        self._logger = logging.getLogger(__name__)
        print("Evaluator: JTFMEvaluator")

    def reset(self):
        self._predictions = []

    def process(self, inputs, outputs):
        """
        Args:
            inputs: the inputs to a model (e.g., GeneralizedRCNN).
                It is a list of dicts. Each dict corresponds to an image and
                contains keys like "height", "width", "file_name", "image_id".
            outputs: the outputs of a model. It is a list of dicts with key
                "instances" that contains :class:`Instances`.
        """
        for input, output in zip(inputs, outputs):
            #Copy some input values to the output
            prediction = {"image_id":input["image_id"], "trow":input["trow"], "tcol":input["tcol"]}

            # TODO this is ugly
            if "instances" in output:
                instances = output["instances"].to(self._cpu_device)
                prediction["instances"] = instances
            else:
                prediction["instances"] = []
            self._predictions.append(prediction)

    def evaluate(self):
        if self._distributed:
            comm.synchronize()
            predictions = comm.gather(self._predictions, dst=0)
            predictions = list(itertools.chain(*predictions)) #Necessary.  I think it joins a set of lists into one

            if not comm.is_main_process():
                return {}
        else:
            predictions = self._predictions

        if len(predictions) == 0:
            self._logger.warning("[COCOEvaluator] Did not receive valid predictions.")
            return {}
        return {"Predictions":predictions}

    def _eval_predictions(self, tasks, predictions):
        """
        Evaluate predictions on the given tasks.
        Fill self._results with the metrics of the tasks.
        """
        pass

# Cell
class Trainer(DefaultTrainer):
    """Customized version of DefaultTrainer, which enables augmentation to be added via
        a custom DatasetMapper during training.  It also optionally uses the Adabound optimizer,
        and it calls build_train_loader, which adds JP_RepeatFactorTrainingSampler.
    """
    #For testing, we don't use augmentation (but see detectron2/tools/train_net.py to add test-time augmentation)
#     @classmethod
#     def build_inference_loader(cls, cfg, dataset_name):
#         return build_test_loader(cfg, dataset_name, mapper=TileDatasetMapper(cfg,False))

    #For training we add image augmentation (=corruption of several kinds)
    @classmethod
    def build_train_loader(cls, cfg):
        return build_train_loader(cfg, mapper=TridentDatasetMapper(cfg, True))

    @classmethod
    def build_test_loader(cls, cfg, dataset_name, dataset_dicts, mapper):
        return build_inference_loader(cfg, dataset_name, dataset_dicts, mapper=TileDatasetMapper)

    @classmethod
    def build_optimizer(cls, cfg, model):
        """
        Builds either a default SGD optimizer or an Adabound optimizer,
        depending on what is specified in configs (cfg.SOLVER.OPTIMIZER_TYPE).
        Returns:
            torch.optim.Optimizer:
        """
        if cfg.SOLVER.OPTIMIZER_TYPE == 'Adabound':
            print("Building Adabound optimizer")
            return build_adabound_optimizer(cfg, model)
        else:
            print("Building default SGD optimizer")
            return build_optimizer(cfg, model) #This is an instance method, called by the class method.


    #Add this method to use a custom optimizer (but how does it interact with momentum, weight decay?)
    def build_adabound_optimizer(cls, cfg, model):
        """
        Build an optimizer from config.  This is built on`detectron2.solver.build_optimizer`,
        but it returns an AdaBound optimizer instead of SGD.
        """
        norm_module_types = (
            nn.BatchNorm1d,
            nn.BatchNorm2d,
            nn.BatchNorm3d,
            nn.SyncBatchNorm,
            # NaiveSyncBatchNorm inherits from BatchNorm2d
            nn.GroupNorm,
            nn.InstanceNorm1d,
            nn.InstanceNorm2d,
            nn.InstanceNorm3d,
            nn.LayerNorm,
            nn.LocalResponseNorm,
        )
        params: List[Dict[str, Any]] = []
        memo: Set[nn.parameter.Parameter] = set()
        for module in model.modules():
            for key, value in module.named_parameters(recurse=False):
                if not value.requires_grad:
                    continue
                # Avoid duplicating parameters
                if value in memo:
                    continue
                memo.add(value)
                lr = cfg.SOLVER.BASE_LR
                weight_decay = cfg.SOLVER.WEIGHT_DECAY
                if isinstance(module, norm_module_types):
                    weight_decay = cfg.SOLVER.WEIGHT_DECAY_NORM
                elif key == "bias":
                    # NOTE: unlike Detectron v1, we now default BIAS_LR_FACTOR to 1.0
                    # and WEIGHT_DECAY_BIAS to WEIGHT_DECAY so that bias optimizer
                    # hyperparameters are by default exactly the same as for regular
                    # weights.
                    lr = cfg.SOLVER.BASE_LR * cfg.SOLVER.BIAS_LR_FACTOR
                    weight_decay = cfg.SOLVER.WEIGHT_DECAY_BIAS
                params += [{"params": [value], "lr": lr, "weight_decay": weight_decay}]

        #optimizer = adabound.AdaBound(model.parameters(), lr=1e-3, final_lr=0.1) #orig
        #print('LR before optimizer: ',cfg.SOLVER.BASE_LR)
        optimizer = AdaBound(params, lr=cfg.SOLVER.BASE_LR, final_lr=0.05)
        optimizer = maybe_add_gradient_clipping(cfg, optimizer)
        #print('LR after optimizer: ',cfg.SOLVER.BASE_LR)
        return optimizer

# Cell
class JP_RepeatFactorTrainingSampler(RepeatFactorTrainingSampler):

    #This is a bug fix (didn't handle empty images correctly)
    @staticmethod
    def repeat_factors_from_category_frequency(dataset_dicts, repeat_thresh):
        """
        Compute (fractional) per-image repeat factors based on category frequency.
        The repeat factor for an image is a function of the frequency of the rarest
        category labeled in that image. The "frequency of category c" in [0, 1] is defined
        as the fraction of images in the training set (without repeats) in which category c
        appears.
        See :paper:`lvis` (>= v2) Appendix B.2.

        Args:
            dataset_dicts (list[dict]): annotations in Detectron2 dataset format.
            repeat_thresh (float): frequency threshold below which data is repeated.
                If the frequency is half of `repeat_thresh`, the image will be
                repeated twice.

        Returns:
            torch.Tensor: the i-th element is the repeat factor for the dataset image
                at index i.
        """
        # 1. For each category c, compute the fraction of images that contain it: f(c)
        category_freq = defaultdict(int)
        for dataset_dict in dataset_dicts:  # For each image (without repeats)
            cat_ids = {ann["category_id"] for ann in dataset_dict["annotations"]}
            for cat_id in cat_ids:
                category_freq[cat_id] += 1
        num_images = len(dataset_dicts)
        for k, v in category_freq.items():
            category_freq[k] = v / num_images

        # 2. For each category c, compute the category-level repeat factor:
        #    r(c) = max(1, sqrt(t / f(c)))
        category_rep = {
            cat_id: max(1.0, math.sqrt(repeat_thresh / cat_freq))
            for cat_id, cat_freq in category_freq.items()
        }

        # 3. For each image I, compute the image-level repeat factor:
        #    r(I) = max_{c in I} r(c)
        # In other words, if the image includes a rare category, you prioritize that and re-sample the image
        # even if it also has common categories in it.
        rep_factors = []
        for dataset_dict in dataset_dicts:
            #Assume empty images are plentiful (don't sample them more often)
            if len(dataset_dict["annotations"]) == 0:
                rep_factors.append(1)
            else:
                cat_ids = {ann["category_id"] for ann in dataset_dict["annotations"]}
                rep_factor = max({category_rep[cat_id] for cat_id in cat_ids})
                rep_factors.append(rep_factor)

        return torch.tensor(rep_factors, dtype=torch.float32)