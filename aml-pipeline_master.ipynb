{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Azure ML Python SDK Master\n",
    "This notebook was compiled from several Microsoft tutorials, with additions from the online documentation and ==>**a good blog post [here](https://medium.com/@santiagof/the-holy-bible-of-azure-machine-learning-service-a-work-through-for-the-believer-part-1-4fe8f9853492)** by a Microsoft developer named Facundo Santiago that gives a nice overview of the AMLS, from which I've also cribbed some bits.\n",
    "\n",
    "**Microsoft's Machine Learning Services can be accesssed (driven) in many different ways**, including:\n",
    "- __The Machine Learning CLI__ is an extension to the Azure CLI. It provides commands for working with the Azure Machine Learning service.\n",
    "- __The Azure Machine Learning SDK for Python__ is a Python package that provides programmatic access to the Azure Machine Learning service.\n",
    "- __The Azure Machine Learning SDK for R__ is an R package that provides programmatic access to the Azure Machine Learning service.\n",
    "- __The Azure Machine Learning Studio__ is a bit like RStudio (i.e., strives to be an IDE with a nice graphical interface).  The documentation says it's currently free for trial but the premium version may cost $10/month plus $1/hour in the future.\n",
    "- __Visual Studio Code__ is an IDE by Microsoft which has a lot of very nice features for programming; there is an extension for MLS.\n",
    "\n",
    "In addition, some computing environments have their own connectors (I think), such as:\n",
    "- __Spark on Databricks__.  A platform for big data, based on Spark.\n",
    "- __Spark on HDInsight__. A cloud distribution of Hadoop components, for processing massive amounts of data using open-source frameworks such as Hadoop, Spark, Hive, LLAP, Kafka, Storm, and R.\n",
    "- __Azure Batch AI__.  A service for running large batch programs.\n",
    "\n",
    "The Python SDK seems to be more capable (as of Jun 2020) than the CLI and less expensive and more flexible than Studio, so **this notebook focuses on the Python SDK**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "* If you are using an Azure Machine Learning Notebook VM, you are all set. Otherwise, go through the [Configuration](../../../configuration.ipynb) notebook to install the Azure Machine Learning Python SDK and create an Azure ML `Workspace`.  See https://docs.microsoft.com/en-us/azure/machine-learning/how-to-configure-environment for configuring your local machine (in addition to a DSVM and other types)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/johntwo/Documents/github/jcpayne/testing_aml_workflow\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SDK version: 1.6.0\n"
     ]
    }
   ],
   "source": [
    "# Check core SDK version number\n",
    "import azureml.core\n",
    "\n",
    "print(\"SDK version:\", azureml.core.VERSION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Diagnostics\n",
    "Opt-in diagnostics for better experience, quality, and security of future releases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {
    "tags": [
     "Diagnostics"
    ]
   },
   "source": [
    "from azureml.telemetry import set_diagnostics_collection\n",
    "\n",
    "set_diagnostics_collection(send_diagnostics=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a workspace\n",
    "A workspace is a logical container for your projects.  You can have many workspaces, and multiple experiments running simultaneously within a workspace. When you create a new workspace, it automatically creates several Azure resources that are used by the workspace:\n",
    "- Azure **Container Registry**: Registers docker containers that you use during training and when you deploy a model. To minimize costs, ACR is lazy-loaded until deployment images are created.\n",
    "- Azure **Storage account**: Is used as the default datastore for the workspace.  By default, both a blob storage and a file share are created (see below).\n",
    "- Azure Application Insights: Stores **monitoring information** about your models.\n",
    "- Azure **Key Vault**: Stores secrets that are used by compute targets and other sensitive information that's needed by the workspace.\n",
    "\n",
    "Best practices for naming:  \n",
    "https://docs.microsoft.com/en-us/azure/cloud-adoption-framework/ready/azure-best-practices/naming-and-tagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a workspace configuration file (once)\n",
    "This code writes a configuration file called `config.json` to `.azureml/config.json`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "configpath.exists()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Workspace\n",
    "\n",
    "subscription_id = '6c7f51dc-3592-4952-bf04-f7b2d747bfca'\n",
    "resource_group  = 'bluedot'\n",
    "workspace_name  = 'jp_workspace'\n",
    "\n",
    "configfile = Path.cwd()/'.azureml/config.json'\n",
    "\n",
    "try:\n",
    "    ws = Workspace(subscription_id = subscription_id, resource_group = resource_group, workspace_name = workspace_name)\n",
    "    if configfile.exists():\n",
    "        print('Existing configuration will be overwritten.')\n",
    "    ws.write_config()\n",
    "    print('Library configuration succeeded')\n",
    "except:\n",
    "    print('Workspace not found')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize workspace\n",
    "Initialize a [Workspace](https://docs.microsoft.com/azure/machine-learning/service/concept-azure-machine-learning-architecture#workspace) object from the existing workspace you created in the Prerequisites step.\n",
    "\n",
    "`Workspace.from_config()` creates a workspace object from the details stored in `config.json`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Workspace name: jp_workspace\n",
      "Azure region: westus2\n",
      "Subscription id: 6c7f51dc-3592-4952-bf04-f7b2d747bfca\n",
      "Resource group: bluedot\n"
     ]
    }
   ],
   "source": [
    "from azureml.core.workspace import Workspace\n",
    "\n",
    "ws = Workspace.from_config()\n",
    "print('Workspace name: ' + ws.name, \n",
    "      'Azure region: ' + ws.location, \n",
    "      'Subscription id: ' + ws.subscription_id, \n",
    "      'Resource group: ' + ws.resource_group, sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create or Attach a compute target\n",
    "Excellent documentation: https://docs.microsoft.com/en-us/azure/machine-learning/how-to-set-up-training-targets.  You can use several different kinds of targets, including:\n",
    "1. **Your local machine**.  The simplest; useful for testing.\n",
    "2. **A managed AML compute cluster** (additional charges for Linux machines).  A cluster can be one or more machines.  This is a slightly more expensive option (I think) that takes some of the hassle out of the next option.  There are two types: _Run-based_ (hardware is deallocated at the end of the run) and _Persisted_, which is ultimately more flexible and useful, since it can scale up and down as needed (you can set the minimum number of nodes to zero). \n",
    "3. **An attached VM**.  ***Only Ubuntu machines are supported***.  This is a \"bring your own\" Azure cloud compute, which could also be a DataBricks cluster, an HDInsight cluster or a Docker VM.  You can use a pre-existing VM (in which case you simply attach to it) or you can create and provision a new VM, then attach it.  Microsoft's DSVMs (Azure Data Science Virtual Machine) are configured well by default.  You can also [create pools of shared DSVMs](https://docs.microsoft.com/en-us/azure/machine-learning/data-science-virtual-machine/dsvm-pools) that can be [automatically scaled](https://docs.microsoft.com/en-us/azure/virtual-machine-scale-sets/virtual-machine-scale-sets-autoscale-overview) to add VMs when the load increases and/or on a schedule; this is basically a version of what Option 2 does.  Also [Azure virtual machine scale sets](https://docs.microsoft.com/en-us/azure/virtual-machine-scale-sets/overview) let you create and manage a group of identical, load balanced VMs, and you can provide your own machine image (you can also create vertically-scaled VM scale sets, which change from weaker to more powerful machines).  I'm not sure about how easy it would be to share a job across them, though. \n",
    "\n",
    "**From Larry O'Brien (author of a lot of the AML docs):**  \n",
    "To train using more-powerful machines, the easy thing to do is use either\n",
    "- Compute _Cluster_ (spun up and down by the training script)\n",
    "- Compute _Instance_ (started and stopped manually)\n",
    " \n",
    "Compute Clusters are generally more appropriate as they’re likely to save you money. Compute Instances are really more about development and iteration than standalone training. For instance, if you wanted to run Jupyter remotely on a powerful machine, you’d choose a Compute Instance. Because they’re manually started and stopped, Compute Instances may be a little more responsive if you are rapidly iterating but Compute Clusters are a little safer in terms of money (they’ll timeout and spin down). You set these choices as the `ComputeTarget` for your training.\n",
    "\n",
    "**WARNING: \"Azure Machine Learning only supports virtual machines that run Ubuntu.** When you create a VM or choose an existing VM, you must select a VM that uses Ubuntu.  Azure Machine Learning also requires the virtual machine to have a public IP address.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alternative 1: Attach your own local machine as the compute target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.runconfig import RunConfiguration\n",
    "\n",
    "# Edit a run configuration property on the fly.\n",
    "run_local = RunConfiguration()\n",
    "\n",
    "run_local.environment.python.user_managed_dependencies = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alternative 2. Attach a managed AML Compute cluster as the compute target\n",
    "This alternative does a little hand-holding to make it simple to set up and scale resources.  Santiago says _Even though Persistent Compute Targets can be created via Python, you will typically provision the target with the Azure Portal because it allows you to better manage your costs and permissions._\n",
    "\n",
    "Regarding environments, you can use a system-built conda environment, an existing Python environment, or a Docker container.  Your VM or docker image _must_ have conda installed.  When you execute by using a Docker container, you need to have Docker Engine running on the VM (obviously). \n",
    "\n",
    "**Creation of AmlCompute takes approximately 5 minutes.** If an AmlCompute with the given name is already in your workspace, this code will skip the creation process.\n",
    "\n",
    "Microsoft limits AmlCompute resources (see [this article](https://docs.microsoft.com/en-us/azure/machine-learning/service/how-to-manage-quotas) on the default limits (which are ZERO GPUs for the NC machines!)).  \n",
    "To request a higher quota:  \n",
    "https://ms.portal.azure.com/#blade/Microsoft_Azure_Support/HelpAndSupportBlade/newsupportrequest/\n",
    "\n",
    "Note: this will check to see if a cluster is already running before creating one.  If it finds a cluster of the same name, it will use it instead of creating a new cluster.\n",
    "\n",
    "Also, if you go to the Compute tab in Studio, you can edit the properties such as the minimum nodes (if 0, the cluster will power down when finished; if 1 it will stay alive).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get a list of supported VM sizes in your region (you'll need the name for the cluster)\n",
    "AmlCompute.supported_vmsizes(ws, location='westus2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.compute import ComputeTarget, AmlCompute\n",
    "from azureml.core.compute_target import ComputeTargetException\n",
    "\n",
    "# choose a name for your cluster\n",
    "cluster_name = \"gpu-cluster\"\n",
    "\n",
    "try:\n",
    "    mycompute_target = ComputeTarget(workspace=ws, name=cluster_name)\n",
    "    print('Found existing compute target.')\n",
    "except ComputeTargetException:\n",
    "    print('Creating a new compute target...')\n",
    "    #max_nodes is the number of VMs you are creating\n",
    "    compute_config = AmlCompute.provisioning_configuration(\n",
    "        vm_size='STANDARD_NC6', \n",
    "        min_nodes = 1, #SET TO 0 FOR IT TO AUTO-STOP, OR TO 1 to keep alive\n",
    "        max_nodes=1) \n",
    "\n",
    "    # create the cluster\n",
    "    mycompute_target = ComputeTarget.create(ws, cluster_name, compute_config)\n",
    "    \n",
    "    #In the estimator, you'll specify\n",
    "    estimator = Estimator(...compute_target=my_compute_target,...)\n",
    "\n",
    "    mycompute_target.wait_for_completion(show_output=True)\n",
    "\n",
    "# use get_status() to get a detailed status for the current cluster. \n",
    "print(mycompute_target.get_status().serialize())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above code creates a GPU cluster. If you instead want to create a CPU cluster, provide a different VM size to the `vm_size` parameter, such as `STANDARD_D2_V2`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alternative 3: Attach a cloud VM\n",
    "Attach: To attach an existing virtual machine as a compute target, you must provide the resource ID, user name, and password for the virtual machine. The resource ID of the VM can be constructed using the subscription ID, resource group name, and VM name using the following string format: \n",
    "resource_id = `/subscriptions/<subscription_id>/resourceGroups/<resource_group>/providers/Microsoft.Compute/virtualMachines/<vm_name>` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.compute import RemoteCompute, ComputeTarget\n",
    "\n",
    "# Create the compute config \n",
    "compute_target_name = \"spearhead-DSVM\"\n",
    "#Create the resource_id\n",
    "resource_id = /subscriptions/6c7f51dc-3592-4952-bf04-f7b2d747bfca/resourceGroups/bluedot/providers/Microsoft.Compute/virtualMachines/Spearhead\n",
    "attach_config = RemoteCompute.attach_configuration(resource_id=resource_id,\n",
    "    ssh_port=22,\n",
    "    username='egdod',\n",
    "    password=None,\n",
    "    private_key_file=\"/Users/johntwo/.ssh/azure_rsa\")\n",
    "    #private_key_passphrase=\"<passphrase>\")\n",
    "\n",
    "# For password access instead of ssh\n",
    "# attach_config = RemoteCompute.attach_configuration(resource_id='<resource_id>',\n",
    "#                                                 ssh_port=22,\n",
    "#                                                 username='<username>',\n",
    "#                                                 password=\"<password>\")\n",
    "\n",
    "# Attach the compute\n",
    "compute = ComputeTarget.attach(ws, compute_target_name, attach_config)\n",
    "\n",
    "compute.wait_for_completion(show_output=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finish creating a run configuration\n",
    "Create a run configuration for the DSVM compute target. Docker and conda are used to create and configure the training environment on the DSVM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import azureml.core\n",
    "from azureml.core.runconfig import RunConfiguration\n",
    "from azureml.core.conda_dependencies import CondaDependencies\n",
    "\n",
    "run_dsvm = RunConfiguration(framework = \"python\")\n",
    "\n",
    "# Set the compute target to the Linux DSVM\n",
    "run_dsvm.target = compute_target_name \n",
    "\n",
    "# Use Docker in the remote VM\n",
    "run_dsvm.environment.docker.enabled = True\n",
    "\n",
    "# Use the CPU base image \n",
    "# To use GPU in DSVM, you must also use the GPU base Docker image \"azureml.core.runconfig.DEFAULT_GPU_IMAGE\"\n",
    "run_dsvm.environment.docker.base_image = azureml.core.runconfig.DEFAULT_GPU_IMAGE\n",
    "#run_dsvm.environment.docker.base_image = azureml.core.runconfig.DEFAULT_CPU_IMAGE\n",
    "print('Base Docker image is:', run_dsvm.environment.docker.base_image)\n",
    "\n",
    "# Specify the CondaDependencies object\n",
    "run_dsvm.environment.python.conda_dependencies = CondaDependencies.create(conda_packages=['scikit-learn'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add data\n",
    "### Overview of data workflow in Azure Machine Learning\n",
    "There is good documentation [here](https://docs.microsoft.com/en-us/azure/machine-learning/concept-data#data-workflow). Generally, the process is:\n",
    "1. **Keep data in some type of data storage _service_** (can be Azure blob, Azure fileshare, PostgreSQL database, external URL, etc.)\n",
    "2. **Register the storage service as an Azure datastore**, which is basically a small file with connection details\n",
    "3. **Create an Azure dataset** from the datastore, which is just packaging to make that datastore consumable by your models.\n",
    "\n",
    "Azure recognizes two types of datasets (which are what they sound like):\n",
    "1. TabularDataset\n",
    "2. FileDataset\n",
    "\n",
    "**Memory limitations**  \n",
    "AML warns that you should have at least twice as much RAM as the amount of data you need to load in memory, and that a CSV file can expand 10X when loaded into RAM and compressed files can expand even further.  \n",
    "- pandas will only use 1 vCPU regardless of how many are available.\n",
    "- You can parallelize pandas by changing `import pandas as pd` to `import modin.pandas as pd`\n",
    "- Otherwise, use Spark, which can run on a cluster if needed (the Python package [Vaex](https://towardsdatascience.com/how-to-process-a-dataframe-with-billions-of-rows-in-seconds-c8212580f447) might also be worth looking at.)\n",
    "\n",
    "**Downloading vs. mounting**  \n",
    "_\"When you mount a dataset, you attach the files referenced by the dataset to a directory (mount point) and make it available on the compute target. When you download a dataset, all the files referenced by the dataset will be downloaded to the compute target.  If your script processes all files referenced by the dataset, and your compute disk can fit your full dataset, downloading is recommended to avoid the overhead of streaming data from storage services. If your data size exceeds the compute disk size, downloading is not possible and we recommend mounting, since only the data files used by your script are loaded at the time of processing._\"\n",
    "\n",
    "Azure ML includes routines and capabilities for:\n",
    "- **[Versioning data](https://docs.microsoft.com/en-us/azure/machine-learning/how-to-version-track-datasets)** in experiments and pipelines.  Note: \"_If you want to make sure that each dataset version is reproducible, we recommend that you not modify data content referenced by the dataset version. When new data comes in, save new data files into a separate data folder and then create a new dataset version to include data from that new folder._\"\n",
    "- **[Labeling images](https://docs.microsoft.com/en-us/azure/machine-learning/how-to-create-labeling-projects)**\n",
    "- **[Detecting \"data drift\"](https://docs.microsoft.com/en-us/azure/machine-learning/how-to-monitor-datasets)**, which might be caused by changing or broken sensors, natural drift like seasonal changes, or change in relationships between covariates.\n",
    "- **Using Azure Open Datasets**, which are \"_curated public datasets that include public-domain data for weather, census, holidays, public safety, and location that help you train machine learning models and enrich predictive solutions._\" \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Automatically-created datastores\n",
    "When you create a workspace, an Azure blob container and an Azure file share are automatically registered to the workspace. They're named `workspaceblobstore` and `workspacefilestore`, respectively. `workspaceblobstore` is used to store workspace artifacts and your machine learning experiment logs. `workspacefilestore` is used to store notebooks and R scripts authorized via compute instance. The `workspaceblobstore` container is set as the default datastore."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataStores \n",
    "A [very useful FAQ](https://docs.microsoft.com/en-us/azure/storage/files/storage-files-faq) that compares the different types of datastore. \n",
    "\n",
    "Azure Files and Azure Blob storage both offer ways to store large amounts of data in the cloud, but they are useful for slightly different purposes.\n",
    "- Azure Blob storage is useful for massive-scale, cloud-native applications that need to store unstructured data. To maximize performance and scale, Azure Blob storage is a simpler storage abstraction than a true file system. __You can access Azure Blob storage only through REST-based client libraries (or directly through the REST-based protocol)__.\n",
    "- Azure Files is specifically a file system. Azure Files has all the file abstracts that you know and love from years of working with on-premises operating systems. Like Azure Blob storage, Azure Files offers a REST interface and REST-based client libraries. Unlike Azure Blob storage, Azure Files offers SMB access to Azure file shares. By using SMB, __you can mount an Azure file share directly on Windows, Linux, or macOS, either on-premises or in cloud VMs, without writing any code or attaching any special drivers to the file system__. You also can cache Azure file shares on on-premises file servers by using Azure File Sync for quick access, close to where the data is used.\n",
    "- Azure Blob storage has higher throughput speeds than an Azure file share and will scale to large numbers of jobs started in parallel. For this reason, we recommend configuring your runs to use Blob storage for transferring source code files. (from https://docs.microsoft.com/en-us/azure/machine-learning/how-to-access-data)\n",
    "- [BlockBlobStorage](https://docs.microsoft.com/en-us/azure/storage/blobs/storage-blob-create-account-block-blob?tabs=azure-portal) is a specialized form of Blob storage with extra-fast access.\n",
    "\n",
    "#### Automatically-created datastores\n",
    "When you create a workspace, an Azure blob container and an Azure file share are automatically registered to the workspace. They're named `workspaceblobstore` and `workspacefilestore`, respectively. `workspaceblobstore` is used to store workspace artifacts and your machine learning experiment logs. `workspacefilestore` is used to store notebooks and R scripts authorized via compute instance. The `workspaceblobstore` container is set as the default datastore."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Writing data\n",
    "See the long discussion I started in Teams about this -- the bottom line is that you should almost always dump results to the `./outputs` folder, which will then get written to storage.  It's unclear to me whether you have to worry about running out of storage if you're generating lots of big files (e.g., making full-size images with a GAN or UNet).  And what exactly _is_ the `./outputs` directory?  \n",
    "\n",
    "Answer from Engineering: `./outputs` \"is usually a blob store mounted with blob fuse, but there is a local disk cache with upload-on-close\".  i.e., it's complicated.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Default names (URI) of storage accounts\n",
    "For example, if your general-purpose storage account is named mystorageaccount, then the default endpoints for that account are:  \n",
    "- Blob storage: https://*mystorageaccount*.blob.core.windows.net\n",
    "- Table storage: https://*mystorageaccount*.table.core.windows.net\n",
    "- Queue storage: https://*mystorageaccount*.queue.core.windows.net\n",
    "- Azure Files: https://*mystorageaccount*.file.core.windows.net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each resource has a corresponding base URI, which refers to the resource itself.\n",
    "- For the storage account, the base URI includes the name of the account only:  \n",
    "    https://myaccount.blob.core.windows.net\n",
    "- For a container, the base URI includes the name of the account and the name of the container:  \n",
    "    https://myaccount.blob.core.windows.net/mycontainer\n",
    "- For a blob, the base URI includes the name of the account, the name of the container, and the name of the blob:  \n",
    "    https://myaccount.blob.core.windows.net/mycontainer/myblob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Secure Access Signature\n",
    "SAS:\n",
    "https://docs.microsoft.com/en-us/azure/storage/common/storage-sas-overview (scroll down to Best Practices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "workspacefilestore AzureFile\n",
      "workspaceblobstore AzureBlob\n"
     ]
    }
   ],
   "source": [
    "#List all datastores registered in the current workspace:\n",
    "datastores = ws.datastores\n",
    "for name, datastore in datastores.items():\n",
    "    print(name, datastore.datastore_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the default datastore\n",
    "datastore = ws.get_default_datastore()\n",
    "\n",
    "#Change the default datastore\n",
    "ws.set_default_datastore(new_default_datastore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Upload data (target_path is root by default)\n",
    "datastore.upload(src_dir='your source file or directory',\n",
    "                 target_path=None,\n",
    "                 overwrite=True,\n",
    "                 show_progress=True)\n",
    "\n",
    "datastore.upload_files() #multiple files\n",
    "\n",
    "#Download.  NOTE: target_path is the local path; prefix is the remote path (should be called 'sourcepath')\n",
    "datastore.download(target_path='your target path',\n",
    "                   prefix='your prefix',\n",
    "                   show_progress=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create and register Datasets\n",
    "See [details](https://docs.microsoft.com/en-us/azure/machine-learning/how-to-create-register-datasets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A single \"Dataset\" can be created from multiple sources.  Generally, the process is:\n",
    "1. Create a list of tuples of the form `(datastore, path)` that specifies the sources\n",
    "2. Use that list to define a dataset.  \n",
    "NOTE: you can use wildcards in the pathnames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Workspace, Datastore, Dataset\n",
    "\n",
    "datastore_name = 'your datastore name'jpworkspace3957796892\n",
    "\n",
    "# get existing workspace\n",
    "workspace = Workspace.from_config()\n",
    "    \n",
    "# retrieve an existing datastore in the workspace by name\n",
    "datastore = Datastore.get(workspace, datastore_name)\n",
    "\n",
    "# create a TabularDataset from 3 file paths in datastore (note multiple sources: one dataset!)\n",
    "datastore_paths = [(datastore, 'weather/2018/11.csv'),\n",
    "                   (datastore, 'weather/2018/12.csv'),\n",
    "                   (datastore, 'weather/2019/*.csv')] #note wildcard\n",
    "weather_ds = Dataset.Tabular.from_delimited_files(path=datastore_paths)\n",
    "\n",
    "# Create a FileDataset from a single source (root directory)\n",
    "datastore_paths = [(datastore, 'TA25')]\n",
    "animal_ds = Dataset.File.from_files(path=datastore_paths)\n",
    "\n",
    "#Create a FileDataset in one step, using wildcards\n",
    "dataset = Dataset.File.from_files('https://dprepdata.blob.core.windows.net/demo/green-small/*.csv')\n",
    "\n",
    "# Create a FileDataset from public image and label files (i.e., data at public web urls)\n",
    "web_paths = ['https://azureopendatastorage.blob.core.windows.net/mnist/train-images-idx3-ubyte.gz',\n",
    "             'https://azureopendatastorage.blob.core.windows.net/mnist/train-labels-idx1-ubyte.gz']\n",
    "mnist_ds = Dataset.File.from_files(path=web_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Register a dataset\n",
    "titanic_ds = titanic_ds.register(workspace=workspace,\n",
    "                                 name='titanic_ds',\n",
    "                                 description='titanic training data')\n",
    "# Register a new version of titanic_ds\n",
    "titanic_ds = titanic_ds.register(workspace = workspace,\n",
    "                                 name = 'titanic_ds',\n",
    "                                 description = 'new titanic training data',\n",
    "                                 create_new_version = True)\n",
    "\n",
    "#Get info about the registration to a workspace \n",
    "image_ds.get_all(ws)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mount or download\n",
    "Note: For mounting a remote file system, Linux uses Blobfuse.  For OS X there is Fuse (https://osxfuse.github.io)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MOUNT (\"mounted_path\" is the mount point on the target machine)\n",
    "import os\n",
    "import tempfile\n",
    "mounted_path = tempfile.mkdtemp() #mkdtemp \"creates a temporary directory in the most secure manner possible.\"\n",
    "\n",
    "# mount dataset onto the mounted_path of a Linux-based compute\n",
    "mount_context = dataset.mount(mounted_path)\n",
    "mount_context.start()\n",
    "\n",
    "print(os.listdir(mounted_path))\n",
    "print (mounted_path)\n",
    "\n",
    "# DOWNLOAD\n",
    "from azureml.opendatasets import MNIST\n",
    "\n",
    "data_folder = os.path.join(os.getcwd(), 'data')\n",
    "os.makedirs(data_folder, exist_ok=True)\n",
    "\n",
    "mnist_file_dataset = MNIST.get_file_dataset()\n",
    "mnist_file_dataset.download(data_folder, overwrite=True)\n",
    "\n",
    "mnist_file_dataset = mnist_file_dataset.register(workspace=ws,\n",
    "                                                 name='mnist_opendataset',\n",
    "                                                 description='training and test dataset',\n",
    "                                                 create_new_version=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Other non-working ways of connecting to a datastore\n",
    "# ds = Datastore.get(ws, datastore_name='workspaceblobstore')\n",
    "# ds_mounted = ds.as_mount()\n",
    "# dp = DataPath.create_from_data_reference(ds_mounted)\n",
    "# rootdir = dp.path_on_datastore()\n",
    "# print('dp',dp)\n",
    "# print('rootdir',rootdir)\n",
    "\n",
    "# with ds.as_mount() as mount_context:\n",
    "#        # list top level mounted files and folders in the dataset\n",
    "#        os.listdir(mount_context.mount_point)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Access data inside a script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pass dataset object as an input with name 'titanic'\n",
    "inputs=[titanic_ds.as_named_input('titanic')],\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "#Create a folder for scripts\n",
    "script_folder = os.path.join(os.getcwd(), \"sklearn-mnist\")\n",
    "os.makedirs(script_folder, exist_ok=True)\n",
    "\n",
    "#This line is like nbdev; it writes the contents of the CELL (not the whole notebook) into a useable file called train.py.\n",
    "%%writefile $script_folder/train.py\n",
    "\n",
    "from azureml.core import Dataset, Run\n",
    "\n",
    "run = Run.get_context()\n",
    "workspace = run.experiment.workspace\n",
    "\n",
    "dataset_name = 'titanic_ds'\n",
    "\n",
    "# Get a dataset by name\n",
    "titanic_ds = Dataset.get_by_name(workspace=workspace, name=dataset_name)\n",
    "\n",
    "# Load a TabularDataset into pandas DataFrame\n",
    "df = titanic_ds.to_pandas_dataframe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model on the remote compute\n",
    "SEE VERY USEFUL notebook [here](https://github.com/Azure/MachineLearningNotebooks/blob/master/tutorials/image-classification-mnist-data/img-classification-part1-training.ipynb)  \n",
    "Now that you have your data and training script prepared, you are ready to train on your remote compute cluster. You can take advantage of Azure compute to leverage GPUs to cut down your training time. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a project directory\n",
    "Create a directory on your local machine that will contain all the necessary code from your local machine that you will need access to on the remote resource. This includes the training script and any additional files your training script depends on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "project_folder = './pytorch-birds'\n",
    "os.makedirs(project_folder, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download training data\n",
    "The dataset we will use (located on a public blob [here](https://azureopendatastorage.blob.core.windows.net/testpublic/temp/fowl_data.zip) as a zip file) consists of about 120 training images each for turkeys and chickens, with 100 validation images for each class. The images are a subset of the [Open Images v5 Dataset](https://storage.googleapis.com/openimages/web/index.html). We will download and extract the dataset as part of our training script `pytorch_train.py`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PREPARE SCRIPT\n",
    "See https://docs.microsoft.com/en-us/azure/machine-learning/tutorial-convert-ml-experiment-to-production?view=azure-ml-py.  It goes through the steps:\n",
    "- Remove non-essential code\n",
    "- Refactor all code into functions\n",
    "- Set up a main() function\n",
    "- Convert Jupyter notebooks to .py files\n",
    "- Combine related functions in .py files\n",
    "- Create unit tests for each file\n",
    "\n",
    "This is for deployment, but may be useful: https://github.com/microsoft/MLOpsPython/blob/master/docs/custom_model.md\n",
    "- Fix directory structure\n",
    "- Update train and/or eval script if needed\n",
    "- Customize the container: https://github.com/microsoft/MLOpsPython/blob/master/docs/custom_container.md\n",
    "    - Provision a container registry\n",
    "    - Create a registry service connection\n",
    "    - Update the environment definition\n",
    "    - Create/modify container build pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parse arguments that are passed in\n",
    "```python\n",
    "#Instantiate the parser\n",
    "parser = argparse.ArgumentParser()\n",
    "\n",
    "#You have to ADD each argument before you can parse it.  \"dest\" just creates a different name for the argument (why would you want that?)\n",
    "parser.add_argument('--data-folder', type=str,dest='data_folder', help='data folder')\n",
    "parser.add_argument(\"--split\", default=\"train\")\n",
    "parser.add_argument(\"--samples\", type=int, default=10)\n",
    "parser.add_argument(\"--scale\", type=float, default=1.0)\n",
    "\n",
    "#Call the parser\n",
    "args = parser.parse_args()\n",
    "\n",
    "#Ways to access it\n",
    "print('Data folder is at:', args.data_folder)\n",
    "print('List all files: ', os.listdir(args.data_folder))\n",
    "X = np.load(os.path.join(args.data_folder, 'features.npy'))\n",
    "y = np.load(os.path.join(args.data_folder, 'labels.npy'))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add logging to training script\n",
    "A \"Run\" will log certain things by default (most files are put in `'./output'` by default), but you have to add code to your script to get specific things added to the output.\n",
    "NOTE: print() statements do produce output (perhaps in the log file?)\n",
    "\n",
    "```python\n",
    "```\n",
    "#### Prepare to log\n",
    "```Python\n",
    "from azureml.core.run import Run\n",
    "\n",
    "#You need this line to get the 'run' object so you can log to it\n",
    "run = Run.get_context()\n",
    "\n",
    "#If you're going to put stuff directly into the special output directory, you make sure it exists (necessary???)\n",
    "os.makedirs('./outputs', exist_ok=True)\n",
    "```\n",
    "#### Log a value, list, tuple, or table\n",
    "```python\n",
    "run.log(name, value, description='') #Log a value\n",
    "run.log_list(name, value, description='') #Log a list\n",
    "run.log_row(name, description=None, **kwargs) #Logs a tuple (if called once) or a table (if called multiple)\n",
    "run.log_table(name, value, description='') #Log a whole table\n",
    "```\n",
    "#### Log an image to the run (either as a file or a Matplotlib plot)\n",
    "```python\n",
    "# Create a plot\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "angle = np.linspace(-3, 3, 50) * scale_factor\n",
    "plt.plot(angle,np.tanh(angle), label='tanh')\n",
    "plt.legend(fontsize=12)\n",
    "plt.title('Hyperbolic Tangent', fontsize=16)\n",
    "plt.grid(True)\n",
    "#Log it\n",
    "run.log_image(name='Hyperbolic Tangent', plot=plt)\n",
    "#To log an arbitrary image file, use the form \n",
    "run.log_image(name, path='./image_path.png')\n",
    "```\n",
    "#### Upload a file into the run artifacts \n",
    "This is only needed if it's not automatically included.\n",
    "```python\n",
    "file_name = 'outputs/myfile.txt'\n",
    "with open(file_name, \"w\") as f:\n",
    "    f.write('A line of output.\\n')\n",
    "run.upload_file(name = file_name, path_or_stream = file_name)\n",
    "```\n",
    "#### Save a snapshot of a whole directory to the run object\n",
    "```python\n",
    "run.take_snapshot('./somedir')\n",
    "```\n",
    "#### End logging\n",
    "```python\n",
    "run.complete() #End logging\n",
    "```\n",
    "Metrics are necessary for hyperparameter tuning (see \"Tune model hyperparameters\" notebook)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert notebook to .py file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jupyter nbconvert --to script <mynotebook>.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### copy it to your project directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "shutil.copy('pytorch_train.py', project_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add logging to training script\n",
    "A \"Run\" will log certain things by default (most files are put in `'./output'` by default), but you have to add code to your script to get specific things added to the output.\n",
    "NOTE: print() statements do produce output (perhaps in the log file?)\n",
    "#### Log a value, list, tuple, or table\n",
    "```Python\n",
    "from azureml.core.run import Run\n",
    "run.log(name, value, description='') #Log a value\n",
    "run.log_list(name, value, description='') #Log a list\n",
    "run.log_row(name, description=None, **kwargs) #Logs a tuple (if called once) or a table (if called multiple)\n",
    "run.log_table(name, value, description='') #Log a whole table\n",
    "```\n",
    "#### Log an image to the run (either as a file or a Matplotlib plot)\n",
    "```python\n",
    "# Create a plot\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "angle = np.linspace(-3, 3, 50) * scale_factor\n",
    "plt.plot(angle,np.tanh(angle), label='tanh')\n",
    "plt.legend(fontsize=12)\n",
    "plt.title('Hyperbolic Tangent', fontsize=16)\n",
    "plt.grid(True)\n",
    "#Log it\n",
    "run.log_image(name='Hyperbolic Tangent', plot=plt)\n",
    "#To log an arbitrary image file, use the form \n",
    "run.log_image(name, path='./image_path.png')\n",
    "```\n",
    "#### Upload a file into the run artifacts \n",
    "This is only needed if it's not automatically included.\n",
    "```python\n",
    "file_name = 'outputs/myfile.txt'\n",
    "with open(file_name, \"w\") as f:\n",
    "    f.write('A line of output.\\n')\n",
    "run.upload_file(name = file_name, path_or_stream = file_name)\n",
    "```\n",
    "#### Save a snapshot of a whole directory to the run object\n",
    "```python\n",
    "run.take_snapshot('./somedir')\n",
    "```\n",
    "#### End logging\n",
    "```python\n",
    "run.complete() #End logging\n",
    "```\n",
    "Metrics are necessary for hyperparameter tuning (see \"Tune model hyperparameters\" notebook)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once your script is ready, **copy it to your project directory**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once your script is ready, **copy it to your project directory**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "shutil.copy('pytorch_train.py', project_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Turn the training notebook into a .py file\n",
    "From the command line:\n",
    "```python\n",
    "#One file:\n",
    "jupyter nbconvert --to script <mynotebook>.ipynb\n",
    "\n",
    "#All notebooks in a directory:\n",
    "jupyter nbconvert --to script /<path>/*.ipynb\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Copy it to the project folder\n",
    "What goes in here?\n",
    "--script file\n",
    "config files should be automatically included by installing the trident package\n",
    "\n",
    "What goes in the data store?\n",
    "--Copy the tiled image files to the blob store\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create an experiment\n",
    "Create an [Experiment](https://docs.microsoft.com/azure/machine-learning/service/concept-azure-machine-learning-architecture#experiment) to track all the runs in your workspace.  An experiment is associated with a workspace, and it is a way to group information from multiple runs.  Each run has an Estimator or ScriptRunConfig that defines a source_folder containing the main script and other assets needed (additional python files, model weights, etc.) From Santiago: _The folder will typically be associated with a code repository. This is not required, but it will allow you to collaborate with others on the same experiment. The repository can be hosted in any service, from GitHub to Azure DevOps._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Experiment\n",
    "\n",
    "experiment_name = 'trident_tz_expt'\n",
    "experiment = Experiment(ws, name=experiment_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environments\n",
    "There are three types of environments:\n",
    "- Curated (for some of the managed services)\n",
    "- User-defined.  Must include `azureml-defaults with version >= 1.0.45` as a pip dependency\n",
    "- System-defined.  Managed by Conda.  This is the default\n",
    "\n",
    "It is not strictly necessary to create an environment, since when you submit a job, one will be created for you.  \n",
    "- Environments are automatically registered to your workspace when you submit an experiment. They can also be manually registered;\n",
    "- You can fetch environments from your workspace to use for training or deployment, or to edit;\n",
    "- With versioning, you can see changes to your environments over time, which ensures reproducibility;\n",
    "- You can build Docker images automatically from your environments.\n",
    "\n",
    "See: https://docs.microsoft.com/en-us/azure/machine-learning/how-to-use-environments for more detail"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create an environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.environment import Environment\n",
    "\n",
    "Environment(name=\"myenv\") #Instantiate an Environment object\n",
    "\n",
    "#Create it from an existing conda environment\n",
    "myenv = Environment.from_existing_conda_environment(name = \"myenv\",\n",
    "                                                    conda_environment_name = \"mycondaenv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add packages to an environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.environment import Environment\n",
    "from azureml.core.conda_dependencies import CondaDependencies\n",
    "\n",
    "myenv = Environment(name=\"myenv\")\n",
    "\n",
    "#Make a section of the environment that holds conda dependencies (see last line for how it's added)\n",
    "conda_dep = CondaDependencies()\n",
    "\n",
    "# Install conda package\n",
    "conda_dep.add_conda_package(\"numpy==1.17.0\")\n",
    "\n",
    "# Install pip package (Note: it's also added to conda dependencies)\n",
    "conda_dep.add_pip_package(\"pillow\")\n",
    "\n",
    "# Adds dependencies to PythonSection of myenv\n",
    "myenv.python.conda_dependencies=conda_dep\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Other environment operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myenv.register(workspace=ws) #Register an environment with a workspace\n",
    "Environment.list(workspace=\"workspace_name\") #List environments associated with a workspace\n",
    "Environment.get(workspace=ws,name=\"myenv\",version=\"1\") #Find an existing environment by name+version\n",
    "Run.get_environment() #Get the environment that was associated with the Run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional: put the environment into a Docker container\n",
    "This is done by default, but you can choose details if you control it.\n",
    "_Note: the string below refers to a \"base image\"_: see https://github.com/Azure/AzureML-Containers.\n",
    "It specifies the CUDA type and the Ubuntu version but not much else."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find the name of your container registry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ws.get_details() #look through it for a registry.\n",
    "#If it exists, the last element of this string like thsi is the container registry name (i.e., 'jpworkspacea09a28ea')\n",
    "#'/subscriptions/6c7f51dc-3592-4952-bf04-f7b2d747bfca/resourceGroups/bluedot/providers/Microsoft.ContainerRegistry/registries/jpworkspacea09a28ea'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates the environment inside a Docker container.\n",
    "myenv.docker.enabled = True\n",
    "\n",
    "# Optional: specify a custom Docker base image and registry if you don't want to use the defaults\n",
    "myenv.docker.base_image=\"your_base-image\"\n",
    "myenv.docker.base_image_registry=\"your_registry_location\"\n",
    "#trident_env.docker.base_image_registry=container_registry\n",
    "\n",
    "#More detail on specifying a container registry\n",
    "#container_registry = ContainerRegistry()\n",
    "#container_registry.address = 'jpworkspacea09a28ea.azurecr.io'\n",
    "#container_registry.username = 'yasiyu'\n",
    "#container_registry.password = registry_pw\n",
    "\n",
    "# Specify docker steps as a string (optional). Alternatively, load the string from a file.\n",
    "dockerfile = r\"\"\"\n",
    "FROM mcr.microsoft.com/azureml/base:intelmpi2018.3-ubuntu16.04\n",
    "RUN echo \"Hello from custom container!\"\n",
    "\"\"\"\n",
    "\n",
    "# Optional: Set base image to None, because the image is defined by dockerfile.\n",
    "myenv.docker.base_image = None\n",
    "myenv.docker.base_dockerfile = dockerfile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare to submit the job\n",
    "**There are several options for submitting jobs, including:**\n",
    "1. **Submit as an `Estimator`**.  An Estimator encapsulates an environment and a target.  There are many types of Estimators\n",
    "2. **Submit as a \"ScriptRunConfig**\"  \n",
    "\n",
    "Santiago recommends Estimator, because it's simpler \"_Estimator is an abstraction that allows you to build a Script Run Configuration based on high-level specifications._\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create an Estimator\n",
    "**An `Estimator` encapsulates an environment and a target**. By default, an Estimator doesn't come with an environment, but Azure makes Estimators for both Tensorflow and Pytorch that come with packages installed. For more information on the PyTorch estimator, refer [here](https://docs.microsoft.com/azure/machine-learning/service/how-to-train-pytorch). Parameters:  \n",
    "- `source_directory` on your local machine must contain all files needed (except data) and must be <300 Mb in size.  It gets copied to a Docker container when the job is run, so you also have to specify:\n",
    "- `entry_script`: the main script;\n",
    "- `script_params`: any command-line params to be added to the main script call;\n",
    "- `compute_target`: the machine(s) to run the job on;\n",
    "- Data can be found and attached or downloaded in the training script, or you can pass the name of a directory\n",
    "- `use_gpu`: Default False. Indicates if the hardware supports GPU. If true, the image deployed in the virtual machine will have all the drivers and distributions to support GPUs.\n",
    "- `use_docker`: Default True. Indicates if the job will be submitted as a Docker image into the compute target.\n",
    "\n",
    "For distributed training, you must 1) specify a cluster as your machine type; then 2) the `distributed_backend` parameter specifies how the nodes of the training cluster will communicate to achieve distributed training.\n",
    "See https://docs.microsoft.com/en-us/azure/machine-learning/how-to-train-pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "dnn-pytorch-remarks-sample"
    ]
   },
   "outputs": [],
   "source": [
    "from azureml.train.dnn import PyTorch\n",
    "import horovod #for distributed training\n",
    "\n",
    "#These are literally command-line parameters to be used with the main .py script file when it is called.\n",
    "#Example 1: point to a local data directory\n",
    "script_params = {\n",
    "    '--data_dir': '/somepath',\n",
    "    '--num_epochs': 30,\n",
    "    '--output_dir': './outputs'\n",
    "}\n",
    "#Example 2. Here, you've already defined ds as a DataStore, so we mount it.\n",
    "script_params = {\n",
    "    '--data-folder': ds.as_mount(),\n",
    "    '--regularization': 0.8\n",
    "}\n",
    "\n",
    "project_folder='./my-proj',\n",
    "\n",
    "#Using the default Estimator() class, which does not have packages installed by default.  \n",
    "estimator = Estimator(source_directory=project_folder,\n",
    "                    entry_script='train.py',              \n",
    "                    script_params=script_params,\n",
    "                    # pass dataset object as an input with name 'titanic'\n",
    "                    inputs=[titanic_ds.as_named_input('titanic')],\n",
    "                    compute_target=compute_target,\n",
    "                    pip_packages=['scikit-learn'],\n",
    "                    conda_packages=['scikit-learn'])\n",
    "\n",
    "#A PyTorch Estimator, containing all of the packages needed for PyTorch\n",
    "estimator = PyTorch(source_directory=project_folder,\n",
    "                    #Here the dataset reference is inside \"script_params\"\n",
    "                    script_params=script_params,\n",
    "                    compute_target=compute_target,\n",
    "                    entry_script='pytorch_train.py',\n",
    "                    use_gpu=True,\n",
    "                    pip_packages=['pillow==5.4.1'])\n",
    "\n",
    "#A Pytorch estimator for distributed training\n",
    "estimator= PyTorch(source_directory=project_folder,\n",
    "                      compute_target=compute_target,\n",
    "                      script_params=script_params,\n",
    "                      entry_script='script.py',\n",
    "                      node_count=2,\n",
    "                      process_count_per_node=1,\n",
    "                      distributed_training=MpiConfiguration(),\n",
    "                      framework_version='1.13',\n",
    "                      use_gpu=True)\n",
    "\n",
    "#A TensorFlow Estimator with all needed TensorFlow packages, here set up for distributed processing. \n",
    "estimator= TensorFlow(source_directory=project_folder,\n",
    "                      compute_target=compute_target,\n",
    "                      script_params=script_params,\n",
    "                      entry_script='tf_horovod_word2vec.py',\n",
    "                      node_count=2,\n",
    "                      process_count_per_node=1,\n",
    "                      distributed_backend='mpi',\n",
    "                      use_gpu=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `script_params` parameter is a dictionary containing the command-line arguments to your training script `entry_script`. Please note the following:\n",
    "- We passed our training data reference `ds_data` to our script's `--data_dir` argument. This will 1) mount our datastore on the remote compute and 2) provide the path to the training data `fowl_data` on our datastore.\n",
    "- We specified the output directory as `./outputs`. The `outputs` directory is specially treated by Azure ML in that all the content in this directory gets uploaded to your workspace as part of your run history. The files written to this directory are therefore accessible even once your remote run is over. In this tutorial, we will save our trained model to this output directory.\n",
    "\n",
    "To leverage the Azure VM's GPU for training, we set `use_gpu=True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.train.estimator import Estimator\n",
    "\n",
    "script_params = {\n",
    "    '--data-folder': ds.as_mount(),\n",
    "    '--regularization': 0.8\n",
    "}\n",
    "\n",
    "sk_est = Estimator(source_directory='./my-sklearn-proj',\n",
    "                   script_params=script_params,\n",
    "                   compute_target=compute_target,\n",
    "                   entry_script='train.py',\n",
    "                   conda_packages=['scikit-learn'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a ScriptRunConfig\n",
    "I think in some ways this may be more flexible because you can create the environment in detail.  See https://docs.microsoft.com/en-us/azure/machine-learning/how-to-set-up-training-targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import ScriptRunConfig, Experiment\n",
    "from azureml.core.environment import Environment\n",
    "\n",
    "exp = Experiment(name=\"myexp\", workspace = ws)\n",
    "# Instantiate environment\n",
    "myenv = Environment(name=\"myenv\")\n",
    "\n",
    "# Add training script to the runconfig.  This is where the source_directory is specified, which may contain other stuff as well.\n",
    "runconfig = ScriptRunConfig(source_directory=\".\", script=\"train.py\")\n",
    "\n",
    "# GPU support: Azure automatically detects and uses the NVIDIA Docker extension when it is available.\n",
    "\n",
    "#For a RunConfig setup:\n",
    "# run_config = RunConfiguration()\n",
    "# run_config.environment.docker.enabled = True\n",
    "# run_config.environment.docker.base_image='tfodapi112:190905'\n",
    "# run_config.environment.docker.base_image_registry=container_registry\n",
    "# run_config.target = compute_target # specify the compute target\n",
    "\n",
    "# Attach compute target to run config\n",
    "runconfig.run_config.target = \"local\"\n",
    "\n",
    "# Attach environment to run config\n",
    "runconfig.run_config.environment = myenv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submit the run\n",
    "As the Run is executed, it goes through the following stages:\n",
    "- **Preparing**: A docker image is created according to the PyTorch estimator. The image is uploaded to the workspace's container registry and cached for later runs. Logs are also streamed to the run history and can be viewed to monitor progress.\n",
    "- **Scaling**: The cluster attempts to scale up if the Batch AI cluster requires more nodes to execute the run than are currently available.\n",
    "- **Running**: All scripts in the script folder are uploaded to the compute target, data stores are mounted or copied, and the entry_script is executed. Outputs from stdout and the ./logs folder are streamed to the run history and can be used to monitor the run.\n",
    "- **Post-Processing**: The ./outputs folder of the run is copied over to the run history.  \n",
    "\n",
    "Note: a `Run` can have children (useful for doing a set of hyperparameter testing runs).  See https://docs.microsoft.com/en-us/azure/machine-learning/how-to-manage-runs for details"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Alternative 1: Running an Estimator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "from azureml.widgets import RunDetails\n",
    "\n",
    "run = <experiment>.submit(<estimator>)\n",
    "RunDetails(run).show() #uses azureml.widgets\n",
    "#Or wait til the run is done:\n",
    "run.wait_for_completion(show_output = True)\n",
    "\n",
    "\n",
    "# to get more details of your run\n",
    "print(run)\n",
    "print(run.get_details())\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Alternative 2: Running a ScriptRunConfig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# Submit run \n",
    "from azureml.widgets import RunDetails\n",
    "run = <experiment>.submit(<runconfig>)\n",
    "\n",
    "#Submit a run on local()\n",
    "script_folder = os.getcwd()\n",
    "run = my_experiment.submit(src)\n",
    "RunDetails(run).show() #uses azureml.widgets\n",
    "#Or wait til the run is done:\n",
    "#run.wait_for_completion(show_output = True)\n",
    "\n",
    "#To run it on a different resource, the only thing you have to change is the run_config.\n",
    "src = ScriptRunConfig(source_directory = script_folder, script = 'train.py', run_config = run_amlcompute)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monitor your run\n",
    "You can monitor the progress of the run with a Jupyter widget. Like the run submission, the widget is asynchronous and provides live updates every 10-15 seconds until the job completes.\n",
    "\n",
    "#### Use a Jupyter widget\n",
    "```python\n",
    "from azureml.widgets import RunDetails\n",
    "RunDetails(run).show()\n",
    "```\n",
    "Alternatively, you can block until the script has completed training before running more code.\n",
    "```python\n",
    "run.wait_for_completion(show_output=True)\n",
    "```\n",
    "#### Get run status:\n",
    "```python\n",
    "#Get run status:\n",
    "print(<run>.get_status())\n",
    "```\n",
    "#### Cancel a run, or mark a run failed\n",
    "```python\n",
    "<run>.cancel() #cancel\n",
    "<run>.fail() #mark failed\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accessing run output\n",
    "#### From the portal\n",
    "You can check output simply by typing:\n",
    "```python\n",
    "run\n",
    "#or\n",
    "experiment\n",
    "```\n",
    "#### From a notebook or script\n",
    "```python\n",
    "# access the run id for use later\n",
    "run_id = run.id\n",
    "\n",
    "#Attach to a run\n",
    "fetched_run = Run(<experiment>, run_id)\n",
    "#or\n",
    "<experiment>.get_runs(<run_id>) #Check the argument -- I'm guessing\n",
    "\n",
    "#Get all run metrics\n",
    "fetched_run.get_metrics()\n",
    "\n",
    "#Get a particular metric\n",
    "fetched_run.get_metrics(name = \"scale factor\")\n",
    "\n",
    "#Get the filenames of files that were uploaded to the run\n",
    "fetched_run.get_file_names()\n",
    "\n",
    "#Download those files to your local machine\n",
    "import os\n",
    "os.makedirs('files', exist_ok=True)\n",
    "\n",
    "for f in run.get_file_names():\n",
    "    dest = os.path.join('files', f.split('/')[-1]) #using just the filename and dropping the path\n",
    "    print('Downloading file {} to {}...'.format(f, dest))\n",
    "    fetched_run.download_file(f, dest)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Register the model\n",
    "Once you've trained the model, you can register it to your workspace. Model registration lets you store and version your models in your workspace to simplify model management and deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = run.register_model(model_name='pt-dnn', model_path='outputs/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tune model hyperparameters\n",
    "Now that we've seen how to do a simple PyTorch training run using the SDK, let's see if we can further improve the accuracy of our model. We can optimize our model's hyperparameters using Azure Machine Learning's hyperparameter tuning capabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start a hyperparameter sweep\n",
    "First, we will define the hyperparameter space to sweep over. Since our training script uses a learning rate schedule to decay the learning rate every several epochs, let's tune the initial learning rate and the momentum parameters. In this example we will use random sampling to try different configuration sets of hyperparameters to maximize our primary metric, the best validation accuracy (`best_val_acc`).\n",
    "\n",
    "Then, we specify the early termination policy to use to early terminate poorly performing runs. Here we use the `BanditPolicy`, which will terminate any run that doesn't fall within the slack factor of our primary evaluation metric. In this tutorial, we will apply this policy every epoch (since we report our `best_val_acc` metric every epoch and `evaluation_interval=1`). Notice we will delay the first policy evaluation until after the first `10` epochs (`delay_evaluation=10`).\n",
    "Refer [here](https://docs.microsoft.com/azure/machine-learning/service/how-to-tune-hyperparameters#specify-an-early-termination-policy) for more information on the BanditPolicy and other policies available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.train.hyperdrive import RandomParameterSampling, BanditPolicy, HyperDriveConfig, uniform, PrimaryMetricGoal\n",
    "\n",
    "param_sampling = RandomParameterSampling( {\n",
    "        'learning_rate': uniform(0.0005, 0.005),\n",
    "        'momentum': uniform(0.9, 0.99)\n",
    "    }\n",
    ")\n",
    "\n",
    "early_termination_policy = BanditPolicy(slack_factor=0.15, evaluation_interval=1, delay_evaluation=10)\n",
    "\n",
    "hyperdrive_config = HyperDriveConfig(estimator=estimator,\n",
    "                                     hyperparameter_sampling=param_sampling, \n",
    "                                     policy=early_termination_policy,\n",
    "                                     primary_metric_name='best_val_acc',\n",
    "                                     primary_metric_goal=PrimaryMetricGoal.MAXIMIZE,\n",
    "                                     max_total_runs=8,\n",
    "                                     max_concurrent_runs=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, lauch the hyperparameter tuning job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start the HyperDrive run\n",
    "hyperdrive_run = experiment.submit(hyperdrive_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monitor HyperDrive runs\n",
    "You can monitor the progress of the runs with the following Jupyter widget. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RunDetails(hyperdrive_run).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or block until the HyperDrive sweep has completed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperdrive_run.wait_for_completion(show_output=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(hyperdrive_run.get_status() == \"Completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Warm start a Hyperparameter Tuning experiment and resuming child runs\n",
    "Often times, finding the best hyperparameter values for your model can be an iterative process, needing multiple tuning runs that learn from previous hyperparameter tuning runs. Reusing knowledge from these previous runs will accelerate the hyperparameter tuning process, thereby reducing the cost of tuning the model and will potentially improve the primary metric of the resulting model. When warm starting a hyperparameter tuning experiment with Bayesian sampling, trials from the previous run will be used as prior knowledge to intelligently pick new samples, so as to improve the primary metric. Additionally, when using Random or Grid sampling, any early termination decisions will leverage metrics from the previous runs to determine poorly performing training runs. \n",
    "\n",
    "Azure Machine Learning allows you to warm start your hyperparameter tuning run by leveraging knowledge from up to 5 previously completed hyperparameter tuning parent runs. \n",
    "\n",
    "Additionally, there might be occasions when individual training runs of a hyperparameter tuning experiment are cancelled due to budget constraints or fail due to other reasons. It is now possible to resume such individual training runs from the last checkpoint (assuming your training script handles checkpoints). Resuming an individual training run will use the same hyperparameter configuration and mount the storage used for that run. The training script should accept the \"--resume-from\" argument, which contains the checkpoint or model files from which to resume the training run. You can also resume individual runs as part of an experiment that spends additional budget on hyperparameter tuning. Any additional budget, after resuming the specified training runs is used for exploring additional configurations.\n",
    "\n",
    "For more information on warm starting and resuming hyperparameter tuning runs, please refer to the [Hyperparameter Tuning for Azure Machine Learning documentation](https://docs.microsoft.com/en-us/azure/machine-learning/service/how-to-tune-hyperparameters) \n",
    "\n",
    "### Find and register the best model\n",
    "Once all the runs complete, we can find the run that produced the model with the highest accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_run = hyperdrive_run.get_best_run_by_primary_metric()\n",
    "best_run_metrics = best_run.get_metrics()\n",
    "print(best_run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Best Run is:\\n  Validation accuracy: {0:.5f} \\n  Learning rate: {1:.5f} \\n  Momentum: {2:.5f}'.format(\n",
    "        best_run_metrics['best_val_acc'][-1],\n",
    "        best_run_metrics['lr'],\n",
    "        best_run_metrics['momentum'])\n",
    "     )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, register the model from your best-performing run to your workspace. The `model_path` parameter takes in the relative path on the remote VM to the model file in your `outputs` directory. In the next section, we will deploy this registered model as a web service."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = best_run.register_model(model_name = 'pytorch-birds', model_path = 'outputs/model.pt')\n",
    "print(model.name, model.id, model.version, sep = '\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipelines\n",
    "Pipelines are an additional piece of complication in this whole workflow, but they offer some advantages:\n",
    "- Unattended execution\n",
    "- Reliably coordinated computation across heterogeneous and scalable computes and storages. \n",
    "- Reusability: They can be triggered from external systems via simple REST calls.\n",
    "- Tracking and versioning of data sources, inputs, and outputs\n",
    "\n",
    "A very simple, easy example of a single pipeline step here:  \n",
    "https://github.com/Azure/MachineLearningNotebooks/blob/master/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-how-to-use-estimatorstep.ipynb  \n",
    "A fuller example here:  \n",
    "https://github.com/Azure/MachineLearningNotebooks/blob/master/how-to-use-azureml/machine-learning-pipelines/parallel-run/file-dataset-image-inference-mnist.ipynb  \n",
    "and here (near the bottom):  \n",
    "https://docs.microsoft.com/en-us/azure/machine-learning/tutorial-pipeline-batch-scoring-classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploy model as web service\n",
    "Once you have your trained model, you can deploy the model on Azure. In this tutorial, we will deploy the model as a web service in [Azure Container Instances](https://docs.microsoft.com/en-us/azure/container-instances/) (ACI). For more information on deploying models using Azure ML, refer [here](https://docs.microsoft.com/azure/machine-learning/service/how-to-deploy-and-where)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create scoring script\n",
    "\n",
    "First, we will create a scoring script that will be invoked by the web service call. Note that the scoring script must have two required functions:\n",
    "* `init()`: In this function, you typically load the model into a `global` object. This function is executed only once when the Docker container is started. \n",
    "* `run(input_data)`: In this function, the model is used to predict a value based on the input data. The input and output typically use JSON as serialization and deserialization format, but you are not limited to that.\n",
    "\n",
    "Refer to the scoring script `pytorch_score.py` for this tutorial. Our web service will use this file to predict whether an image is a chicken or a turkey. When writing your own scoring script, don't forget to test it locally first before you go and deploy the web service."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create environment file\n",
    "Then, we will need to create an environment file (`myenv.yml`) that specifies all of the scoring script's package dependencies. This file is used to ensure that all of those dependencies are installed in the Docker image by Azure ML. In this case, we need to specify `azureml-core`, `torch` and `torchvision`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.conda_dependencies import CondaDependencies \n",
    "\n",
    "myenv = CondaDependencies.create(pip_packages=['azureml-defaults', 'torch', 'torchvision>=0.5.0'])\n",
    "\n",
    "with open(\"myenv.yml\",\"w\") as f:\n",
    "    f.write(myenv.serialize_to_string())\n",
    "    \n",
    "print(myenv.serialize_to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deploy to ACI container\n",
    "We are ready to deploy. Create an inference configuration which gives specifies the inferencing environment and scripts. Create a deployment configuration file to specify the number of CPUs and gigabytes of RAM needed for your ACI container. While it depends on your model, the default of `1` core and `1` gigabyte of RAM is usually sufficient for many models. This cell will run for about 7-8 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.webservice import AciWebservice\n",
    "from azureml.core.model import InferenceConfig\n",
    "from azureml.core.webservice import Webservice\n",
    "from azureml.core.model import Model\n",
    "from azureml.core.environment import Environment\n",
    "\n",
    "\n",
    "myenv = Environment.from_conda_specification(name=\"myenv\", file_path=\"myenv.yml\")\n",
    "inference_config = InferenceConfig(entry_script=\"pytorch_score.py\", environment=myenv)\n",
    "\n",
    "aciconfig = AciWebservice.deploy_configuration(cpu_cores=1, \n",
    "                                               memory_gb=1, \n",
    "                                               tags={'data': 'birds',  'method':'transfer learning', 'framework':'pytorch'},\n",
    "                                               description='Classify turkey/chickens using transfer learning with PyTorch')\n",
    "\n",
    "service = Model.deploy(workspace=ws, \n",
    "                           name='aci-birds', \n",
    "                           models=[model], \n",
    "                           inference_config=inference_config, \n",
    "                           deployment_config=aciconfig)\n",
    "service.wait_for_deployment(True)\n",
    "print(service.state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If your deployment fails for any reason and you need to redeploy, make sure to delete the service before you do so: `service.delete()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tip: If something goes wrong with the deployment, the first thing to look at is the logs from the service by running the following command:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "service.get_logs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the web service's HTTP endpoint, which accepts REST client calls. This endpoint can be shared with anyone who wants to test the web service or integrate it into an application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(service.scoring_uri)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the web service\n",
    "Finally, let's test our deployed web service. We will send the data as a JSON string to the web service hosted in ACI and use the SDK's `run` API to invoke the service. Here we will take an image from our validation data to predict on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "plt.imshow(Image.open('test_img.jpg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import transforms\n",
    "    \n",
    "def preprocess(image_file):\n",
    "    \"\"\"Preprocess the input image.\"\"\"\n",
    "    data_transforms = transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "    image = Image.open(image_file)\n",
    "    image = data_transforms(image).float()\n",
    "    image = torch.tensor(image)\n",
    "    image = image.unsqueeze(0)\n",
    "    return image.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = preprocess('test_img.jpg')\n",
    "result = service.run(input_data=json.dumps({'data': input_data.tolist()}))\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean up\n",
    "Once you no longer need the web service, you can delete it with a simple API call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "service.delete()"
   ]
  }
 ],
 "metadata": {
  "authors": [
   {
    "name": "swatig"
   }
  ],
  "category": "training",
  "compute": [
   "AML Compute"
  ],
  "datasets": [
   "ImageNet"
  ],
  "deployment": [
   "Azure Container Instance"
  ],
  "exclude_from_index": false,
  "framework": [
   "PyTorch"
  ],
  "friendly_name": "Training with hyperparameter tuning using PyTorch",
  "index_order": 1,
  "kernelspec": {
   "display_name": "Python [conda env:fastai]",
   "language": "python",
   "name": "conda-env-fastai-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "tags": [
   "None"
  ],
  "task": "Train an image classification model using transfer learning with the PyTorch estimator"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
