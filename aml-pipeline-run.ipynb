{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SDK version: 1.6.0\n"
     ]
    }
   ],
   "source": [
    "# Check core SDK version number\n",
    "import azureml.core\n",
    "from azureml.core import Workspace, Datastore, Dataset, Environment, Experiment, Model\n",
    "from azureml.core.run import Run\n",
    "from azureml.core.runconfig import RunConfiguration\n",
    "from azureml.core.compute import ComputeTarget, AmlCompute, RemoteCompute, ComputeTarget\n",
    "from azureml.core.compute_target import ComputeTargetException\n",
    "from azureml.train.estimator import Estimator\n",
    "from azureml.core.conda_dependencies import CondaDependencies\n",
    "from azureml.data.datapath import DataPath\n",
    "\n",
    "#Other stuff\n",
    "import os\n",
    "import glob\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "print(\"SDK version:\", azureml.core.VERSION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.workspace import Workspace\n",
    "\n",
    "ws = Workspace.from_config()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IMPORTANT REMINDER about versions \n",
    "**The version of Detectron2 that runs on the AML cluster is behind the one that runs on the VM (Spearhead3).\n",
    "Model weights are probably not transferable between the cluster and Spearhead3.**  I can't update the cluster version because _the cluster machines use CUDA 10.1._  I may be able to downgrade Spearhead3's CUDA version in the future, but it's still useful for other development (tiling, etc.).\n",
    "\n",
    "**Differences between versions of Detectron2:**  \n",
    "- This version uses TransformGen, which has been renamed to Augmentation in the newer version on Spearhead3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Copy data from AWS to the Azure VM (if necessary)\n",
    "See `trident_tz_on_vm.ipynb` for details.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Copy data from the VM to the workspace blobstore (if necessary)\n",
    "See `trident_tz_on_vm.ipynb` for details.  Remember that you only need to copy tiled images and annotations; the originals don't matter for the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remake a Dataset (if necessary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#REMAKE A DATASET\n",
    "# # retrieve an existing datastore in the workspace by name\n",
    "datastore = Datastore.get(ws, 'workspaceblobstore')\n",
    "\n",
    "# #A \"FileDataset\" is really just a name for a blob, and nothing happens until it's mounted.\n",
    "# #Once it's mounted, then you have to use other tools to get its paths \n",
    "# #Define a FileDataset for the temp directory, which has subdirectories\n",
    "datastore_paths = [(datastore, 'temp')]\n",
    "image_ds = Dataset.File.from_files(path=datastore_paths)\n",
    "\n",
    "#Register it \n",
    "image_ds = image_ds.register(workspace=ws,\n",
    "                                 name='image_ds',\n",
    "                                 description='tanzania main folder temp',\n",
    "                                 create_new_version=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare for run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check workspace and datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Workspace name: ' + ws.name, \n",
    "      'Azure region: ' + ws.location, \n",
    "      'Subscription id: ' + ws.subscription_id, \n",
    "      'Resource group: ' + ws.resource_group, sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#List all datastores registered in the current workspace:\n",
    "datastores = ws.datastores\n",
    "for name, datastore in datastores.items():\n",
    "    print(name, datastore, datastore.datastore_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check the image data\n",
    "1. You can see what files are in the folder using Microsoft Storage Explorer\n",
    "2. The following commands are slow to execute, but the number of filenames should match the size of the temp directory (in Storage Explorer, right-click on the folder and then \"Selection Statistics\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/annotations/TA25-RKE-20191128A/TA25-RKE-20191128A_L_4766.xml',\n",
       " '/annotations/TA25-RKE-20191128A/TA25-RKE-20191128A_L_4767.xml',\n",
       " '/annotations/TA25-RKE-20191128A/TA25-RKE-20191128A_L_4768.xml',\n",
       " '/annotations/TA25-RKE-20191128A/TA25-RKE-20191128A_L_4769.xml',\n",
       " '/annotations/TA25-RKE-20191128A/TA25-RKE-20191128A_L_4770.xml']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Warning -- these are SLOW to execute\n",
    "#fs= image_ds.to_path() #Get all filenames\n",
    "\n",
    "#Count filenames\n",
    "len(image_ds.to_path()) #15602\n",
    "#Take a sample of 5\n",
    "image_ds.take(5).to_path()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy a cluster\n",
    "Note: this will check to see if a cluster is already running before creating one.  If it finds a cluster of the same name, it will use it instead of creating a new cluster.\n",
    "\n",
    "Also, if you go to the Compute tab in Studio, you can edit the properties such as the minimum nodes (if 0, the cluster will power down when finished; if 1 it will stay alive)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get a list of supported VM sizes in your region (you'll need the name for the cluster)\n",
    "AmlCompute.supported_vmsizes(ws, location='westus2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating a new compute target...\n",
      "Creating\n",
      "Succeeded\n",
      "AmlCompute wait for completion finished\n",
      "\n",
      "Minimum number of nodes requested have been provisioned\n"
     ]
    }
   ],
   "source": [
    "from azureml.core.compute import ComputeTarget, AmlCompute\n",
    "from azureml.core.compute_target import ComputeTargetException\n",
    "\n",
    "# choose a name for your cluster\n",
    "cluster_name = \"gpu-cluster\"\n",
    "\n",
    "try:\n",
    "    mycompute_target = ComputeTarget(workspace=ws, name=cluster_name)\n",
    "    print('Found existing compute target.')\n",
    "except ComputeTargetException:\n",
    "    print('Creating a new compute target...')\n",
    "    #max_nodes is the number of VMs you are creating\n",
    "    #Try NC24r_Promo\n",
    "    compute_config = AmlCompute.provisioning_configuration(\n",
    "        vm_size= 'Standard_NC24rs_v2',#'Standard_NC24r','STANDARD_NC12S_V2',#'Standard_NC24r','STANDARD_NC6'\n",
    "        min_nodes = 0,#SET TO 0 FOR IT TO AUTO-STOP, OR TO 1 to keep alive\n",
    "        max_nodes=1) \n",
    "\n",
    "    # create the cluster\n",
    "    mycompute_target = ComputeTarget.create(ws, cluster_name, compute_config)\n",
    "    \n",
    "    mycompute_target.wait_for_completion(show_output=True)\n",
    "\n",
    "# use get_status() to get a detailed status for the current cluster. \n",
    "# print(mycompute_target.get_status().serialize())\n",
    "\n",
    "#WAIT for it to say \"Minimum number of nodes requested have been provisioned\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attach VM instead of cluster\n",
    "**REMINDER -- Spearhead3 uses CUDA v10.2 whereas the containerized code uses CUDA v10.1, so Spearhead3 should not be used for model training because the weights may not be transferrable.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.compute import RemoteCompute, ComputeTarget\n",
    "\n",
    "# Create the compute config \n",
    "compute_target_name = \"spearhead3\"\n",
    "#Create the resource_id (fill in your own values)\n",
    "resource_id = /subscriptions/<subscription_id>/resourceGroups/<resource_group>/providers/Microsoft.Compute/virtualMachines/<vm_name>\n",
    "attach_config = RemoteCompute.attach_configuration(resource_id=resource_id,\n",
    "    ssh_port=22,\n",
    "    username=<user>,\n",
    "    password=None,\n",
    "    private_key_file=<path/to/.ssh/azure_rsa>)\n",
    "    #private_key_passphrase=\"<passphrase>\")\n",
    "\n",
    "# For password access instead of ssh\n",
    "# attach_config = RemoteCompute.attach_configuration(resource_id='<resource_id>',\n",
    "#                                                 ssh_port=22,\n",
    "#                                                 username='<username>',\n",
    "#                                                 password=\"<password>\")\n",
    "\n",
    "# Attach the compute\n",
    "spearhead3_compute_target = ComputeTarget.attach(ws, compute_target_name, attach_config)\n",
    "compute.wait_for_completion(show_output=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create or get an experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This \"gets OR creates\" an experiment; i.e., if it already exists, it will return the existing one\n",
    "#from azureml.core import Experiment\n",
    "\n",
    "experiment_name = 'aisurvey_expt4'\n",
    "experiment = Experiment(ws, name=experiment_name)\n",
    "experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trident_env = Environment(name=\"trident\") #Instantiate an Environment object\n",
    "trident_env.python.user_managed_dependencies = True\n",
    "trident_env.docker.enabled = True\n",
    "trident_env.docker.base_image = 'tridentbase:v5'\n",
    "trident_env.docker.base_image_registry.address = <your_workspace_container_registry>\n",
    "#Get the existing conda dependencies\n",
    "conda_dep = trident_env.python.conda_dependencies\n",
    "#azureml-defaults seems to be added automatically, even if you don't include it anywhere\n",
    "\n",
    "#Register the environment with the workspace\n",
    "trident_env.register(ws)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "trident_env = Environment.get(ws,\"trident\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make sure I have the dataset needed (this is a repeat of above)\n",
    "image_ds = Dataset.get_by_name(ws, name='image_ds') #or Dataset.get_by_id(ws,id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "tags": [
     "dnn-pytorch-remarks-sample"
    ]
   },
   "outputs": [],
   "source": [
    "#These are command-line parameters to be used with the main .py script file when it is called.\n",
    "#Notes:\n",
    "# 1: as_named_input() looks for a dataset registered to the workspace under that name\n",
    "# 2: \"data_folder at this stage is a \"DatasetConsumptionConfig\" object.  It gets converted during the run to a directory name.\n",
    "# 3: dist_url should be set to \"auto\" for a single machine, or a master node URL for multiple machines.\n",
    "script_params = {\n",
    "    # Mount image_ds and pass the mount point name as the data folder argument \n",
    "    '--data_folder': image_ds.as_named_input('image_ds').as_mount(),\n",
    "    '--output_dir':  './outputs',\n",
    "    '--num_gpus_per_machine':4,\n",
    "    '--num_machines':1,\n",
    "    '--machine_rank':0,\n",
    "    '--dist_url':\"auto\" #, \n",
    "    #'--eval_only':None # Will trigger evaluation if uncommented (yes, 'None' is correct; 'True' throws an error)\n",
    "}\n",
    "\n",
    "project_folder = './trident_project'\n",
    "est = Estimator(source_directory=project_folder,\n",
    "              script_params=script_params,\n",
    "              compute_target=mycompute_target,\n",
    "              environment_definition=Environment.get(workspace=ws,name=\"trident\"),\n",
    "              entry_script='trident_run.py')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Did you copy the weights over AND double-check the filename in test_predict.py?\n",
    "Using Azure Storage Explorer:   \n",
    "- go to the usual `azureml-blobstore_etc`, \n",
    "- then to `azureml`, \n",
    "- then find the run you want (usually on pg 2 -- look for where the folders end), \n",
    "- then copy `model_final.pth` into `azureml-blobstore_etc/temp` (NOT outside `temp`!).  Moving doesn't work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "run = experiment.submit(est)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'aisurvey_expt5_1596913142_d733d47c'"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Get the runId of the current run so you can load it later\n",
    "run.get_details()['runId']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Then come back tomorrow and see how it's doing:\n",
    "run = Run(exp, 'mdv4_trial_1580850141_379ed8f0')\n",
    "run.get_status()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Live-stream logs (every 10-15 seconds)\n",
    "from azureml.widgets import RunDetails\n",
    "RunDetails(run).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assess training progress"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Understanding the Detectron2 output\n",
    "Reminder: don't interpret the slash in the name as division: it just identifies a model component.  For example, `fast_rcnn/cls_accuracy` means the \"class accuracy of the RCNN\"\n",
    "- `fast_rcnn/cls_accuracy` = num_accurate / num_instances (Want this to be 1)\n",
    "- `fast_rcnn/fg_cls_accuracy` = fg_num_accurate / num_fg (Want this to be 1)\n",
    "- `fast_rcnn/false_negative` = num_false_negative / num_fg) (Want this to be zero)\n",
    "- `roi_head/num_fg_samples` The number of fg samples that are selected for training ROI heads =mean(num_fg_samples)\n",
    "- `roi_head/num_bg_samples` The number of bg samples that are selected for training ROI heads =mean(num_bg_samples)\n",
    "- `rpn/num_pos_anchors` the number of positive anchors per-image used in training = num_pos_anchors/num_images\n",
    "- `rpn/num_neg_anchors` the number of negative anchors per-image used in training = num_neg_anchors/num_images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Data folder': '/tmp/tmpq86gb_ai',\n",
       " 'annotationpath': \"PosixPath('/tmp/tmpq86gb_ai/tiled_annotations')\",\n",
       " 'test': 100,\n",
       " 'rootdir': \"PosixPath('/tmp/tmpq86gb_ai')\",\n",
       " 'test_list': [1, 2, 3],\n",
       " 'imagepath': \"PosixPath('/tmp/tmpq86gb_ai/tiled_images')\",\n",
       " 'output_dir': './outputs',\n",
       " 'Class names': ['giraffe',\n",
       "  'building',\n",
       "  'cow',\n",
       "  'human',\n",
       "  'impala',\n",
       "  'buffalo',\n",
       "  'elephant',\n",
       "  'boma',\n",
       "  'shoats',\n",
       "  'zebra',\n",
       "  'donkey',\n",
       "  'wildebeest',\n",
       "  'oryx',\n",
       "  'charcoal sack',\n",
       "  'charcoal mound',\n",
       "  'eland',\n",
       "  'kudu',\n",
       "  'hartebeest',\n",
       "  'gazelle'],\n",
       " 'survey_valid': \"Metadata(name='survey_valid', thing_classes=['giraffe', 'building', 'cow', 'human', 'impala', 'buffalo', 'elephant', 'boma', 'shoats', 'zebra', 'donkey', 'wildebeest', 'oryx', 'charcoal sack', 'charcoal mound', 'eland', 'kudu', 'hartebeest', 'gazelle'])\"}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#This doesn't show metrics because Detectron stores them in metrics.json, which is only available after the run.\n",
    "run.get_metrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get files associated with the run\n",
    "print(run.get_file_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Where the hell are my output files?\n",
    "Answer: use the Microsoft Azure Storage Explorer.  It's in:\n",
    "- `azureml-blobstore-84d56c80-a8c3-4b14-a1fa-0dde9dadda0d/azureml/<run_results>/outputs`  \n",
    "For `<run_results>`, choose one of the folders, noting that:   \n",
    "1. The files are in backwards date order (oldest on top)\n",
    "2. There are TWO folders for each run; one of them has a name that ends in '-setup' and the other is the results\n",
    "3. There are also two files per run; one is a zip and the other has 0 bytes; I think the zip probably has everything but I haven't checked."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Register the model\n",
    "This makes the workspace track it and version it.  It's also a step necessary for deployment.  There's nice documentation [here](https://docs.microsoft.com/en-us/azure/machine-learning/tutorial-train-models-with-aml) and in part 2 of the same tutorial they do deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tridentmodelv1\ttridentmodelv1:1\t1\n"
     ]
    }
   ],
   "source": [
    "from azureml.core.model import Model\n",
    "# Tip: When model_path is set to a directory, you can use the child_paths parameter to include\n",
    "#      only some of the files from the directory\n",
    "# model = Model.register(model_path = \"./models\",\n",
    "#                        model_name = \"sentiment\",\n",
    "#                        description = \"Sentiment analysis model trained outside Azure Machine Learning\",\n",
    "#                        workspace = ws)\n",
    "# Register model\n",
    "model = run.register_model(model_name='tridentmodelv1',\n",
    "                           model_path='outputs/model_final.pth',\n",
    "                           description = \"First training on v2s 6Aug2020\")\n",
    "print(model.name, model.id, model.version, sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Other Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload files to workspace datastore\n",
    "**Notes:**\n",
    "1. The target_path parameter specifies the location in the file share (or blob container) to upload. It defaults to `None`, so the data is uploaded to root. If overwrite=True, any existing data at target_path is overwritten.\n",
    "2. You can also upload a list of individual files to the datastore via the `upload_files()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Upload either a directory or individual files to the datastore by using the Python SDK:\n",
    "datastore = Datastore.get(ws, datastore_name='workspaceblobstore')\n",
    "datastore.upload(src_dir='your source directory',\n",
    "                 target_path='your target path',\n",
    "                 overwrite=True,\n",
    "                 show_progress=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try to access Howard's DataShare (testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'image_ds': DatasetRegistration(id='3f3064ba-5016-4255-90b7-72d4e528e94a', name='image_ds', version=2, description='tanzania main folder temp', tags={})}"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_ds.get_all(ws)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_ds = Dataset.get_by_name(workspace=ws, name='image_ds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tanz_ds = Dataset.get_by_name(workspace=ws, name='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "datastore = Datastore.get(ws, 'workspaceblobstore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\n",
       "  \"name\": \"workspaceblobstore\",\n",
       "  \"container_name\": \"azureml-blobstore-84d56c80-a8c3-4b14-a1fa-0dde9dadda0d\",\n",
       "  \"account_name\": \"jpworkspace3957796892\",\n",
       "  \"protocol\": \"https\",\n",
       "  \"endpoint\": \"core.windows.net\"\n",
       "}"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datastore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "#datastore_paths = 'https://datashare.hosting.portal.azure.net/account/receivedshares/sharesubscriptiondetails#'\n",
    "\n",
    "#[(datastore, 'surveyimagery_hlf')] #[(datastore, 'sde-images')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "tanz_ds = Dataset.File.from_files(path=datastore_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/sharesubscriptiondetails#']"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tanz_ds.take(5).to_path()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://datashare.hosting.portal.azure.net/account/receivedshares/sharesubscriptiondetails#\n",
    "#from azureml.core import Workspace, Datastore, Dataset\n",
    "\n",
    "# retrieve an existing datastore in the workspace by name\n",
    "datastore = Datastore.get(workspace, workspaceblobstore)\n",
    "\n",
    "# create a TabularDataset from 3 file paths in datastore (note multiple sources: one dataset!)\n",
    "datastore_paths = [(datastore, 'weather/2018/11.csv'),\n",
    "                   (datastore, 'weather/2018/12.csv'),\n",
    "                   (datastore, 'weather/2019/*.csv')] #note wildcard\n",
    "weather_ds = Dataset.Tabular.from_delimited_files(path=datastore_paths)\n",
    "\n",
    "# Create a FileDataset from a single source (root directory)\n",
    "datastore_paths = [(datastore, 'TA25')]\n",
    "animal_ds = Dataset.File.from_files(path=datastore_paths)\n",
    "\n",
    "#Create a FileDataset in one step, using wildcards\n",
    "dataset = Dataset.File.from_files('https://dprepdata.blob.core.windows.net/demo/green-small/*.csv')\n",
    "\n",
    "# Create a FileDataset from public image and label files (i.e., data at public web urls)\n",
    "web_paths = ['https://azureopendatastorage.blob.core.windows.net/mnist/train-images-idx3-ubyte.gz',\n",
    "             'https://azureopendatastorage.blob.core.windows.net/mnist/train-labels-idx1-ubyte.gz']\n",
    "mnist_ds = Dataset.File.from_files(path=web_paths)\n",
    "\n",
    "hdata = Dataset.get_by_name(ws,name='sde-images')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How I added AdaBound optimizer\n",
    "\"As described in the paper, AdaBound is an optimizer that behaves like Adam at the beginning of training, and gradually transforms to SGD at the end. The final_lr parameter indicates AdaBound would transforms to an SGD with this learning rate. **In common cases, a default final learning rate of 0.1 can achieve relatively good and stable results on unseen data. It is not very sensitive to its hyperparameters.** See Appendix G of the paper for more details.\"\n",
    "\n",
    "To AdaBound, I added the following to `test_predict.py`:\n",
    "1. imports for the `adabound` package\n",
    "2. imports for some supporting packages in `detectron2.solver.build`\n",
    "3. A new class method (`build_optimizer`) added to the Trainer class.\n",
    "\n",
    "Using conda_dep to import the package failed, so I added it to the Docker image instead, which worked, and the changes moved the Docker image on to v5.  \n",
    "`conda_dep.add_pip_package(\"adabound\") #FAILED`  \n",
    "See aml-pipeline_tanzania.ipynb for Dockerfile details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training loop\n",
    "\n",
    "- `detectron2/tools/plain_train_net.py` is a fairly fleshed-out version of a training script that you might want to customize.\n",
    "- `train_net.py` is a simpler version with more default behavior baked in.\n",
    "\n",
    "The `DefaultTrainer` class has a good layout of everything you'd want to overwrite.  It's in `detectron2/engine/Defaults.py`.  \n",
    "\n",
    "`DefaultTrainer` calls its superclass `SimpleTrainer`, which is:   \n",
    "\"A simple trainer for the most common type of task: single-cost single-optimizer single-data-source iterative optimization.  It assumes that every step, you:  \n",
    "1. Compute the loss with a data from the data_loader.  \n",
    "2. Compute the gradients with the above loss.  \n",
    "3. Update the model with the optimizer.  \n",
    "`SimpleTrainer` includes the method `run_step(),` which runs a _single_ step.\n",
    "\n",
    "All other tasks during training (checkpointing, logging, evaluation, LR schedule) are maintained by hooks, which can be registered by `TrainerBase.register_hooks`.\"\n",
    "\n",
    "The actual training _loop_ function is in its superclass, `TrainerBase`.  All `train()` does is step for max_iterations, and processes hooks (`before_step`, `after_step`, etc.).\n",
    "```python\n",
    "def train():\n",
    "    for self.iter in range(start_iter, max_iter):\n",
    "        self.before_step()\n",
    "        self.run_step()\n",
    "        self.after_step()\n",
    "```\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:fastai]",
   "language": "python",
   "name": "conda-env-fastai-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
