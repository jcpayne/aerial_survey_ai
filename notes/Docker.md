This is one in a series of notes that I keep for myself.  I haven't maintained attribution for all of its pieces, but it is basically an edited selection of quotes from the web, pieces of official documentation, answers to StackExchange questions, forum conversations, blog posts, and so on, which I organize in a way that makes sense to me.  I hope that it is reasonably accurate and that it may be useful to others.

John Payne

## Introduction
#### What is Docker?
Docker is a platform that runs an application in a segregated and secure environment via the use of its kernel containerization feature. It is a highly efficient and lightweight platform in terms of resource use. It uses the host underlying kernel containerization feature rather than creating its own hypervisor.  As docker containers are entirely autonomous, they render essential service components by providing strong isolation, thereby ensuring that containers don't interfere with one another, or with the server wherein they have been configured.  As per Docker claims, the units have the strongest isolation capability in the industry existing today.

#### Docker Image vs. Container
**A Docker image is a single, non-changeable, read-only file containing libraries, source code, tools and other files needed to run applications**.  Docker images are also referred to as snapshots and the ability ability to timestamp them allows developers to test apps under various conditions.  Because images are basically templates, you cannot run or execute them. But you can use a template to build a container on top of it. 
**A Docker Container is the instantiation of a Docker Image**. Every time you create a container, it creates a writable layer right on top of an unchangeable image, and you can now modify it the way you want.

#### Docker Image vs. VM image
Unlike VMs, where virtualization takes place in the hardware, containers are virtualized in the app layer.  This is often called OS-level virtualization and its only downside is that both host and guest OS need to be alike. However, they can make use of one machine, partition its kernel, virtualize the operating system and run isolated processes as if the fellow OS never existed.

A Docker image consists of read-only layers each of which represents a Dockerfile instruction. The layers are stacked and each one is a delta of the changes from the previous layer.  Therefore the idea of a cache is important: Docker will avoid re-creating anything it thinks already exists, so as it goes through commands in the Dockerfile, it always checks the cache before executing a command.  This is both good and bad, since it can be a source of confusion.

#### Volumes vs Bind mounts
Volumes are the preferred mechanism for persisting data generated by and used by Docker containers. While bind mounts are dependent on the directory structure of the host machine, volumes are completely managed by Docker. Volumes have several advantages over bind mounts, but bind mounts may be used in development.   See https://docs.docker.com/storage/volumes/


## Setup
#### Is Docker running and can you reach it?
**WARNING: On a Mac, the Docker app must be running to do anything** (little whale on top bar).  'docker version' will return a result even if the app isn't running, but _nothing else will work._
`docker run hello-world` Should work
`docker info`
`docker version`

#### Fix the permission problem
By default, Docker runs only on the root user.  This solution is in the documentation.
```bash
sudo groupadd docker #Add a docker group
sudo usermod -aG docker $USER #Add your user to it
newgrp docker 
```
#### Change where Docker stores images
(critical for Azure DSVM’s, which have small OS disks).  See [here](https://www.guguweb.com/2019/02/07/how-to-move-docker-data-directory-to-another-location-on-ubuntu/)
`sudo service docker stop`
`vim /etc/docker/daemon.json`
Change this line to fix the path (leave everything else as-is)
```json
{ 
   "graph": "/cdata/docker" 
}
```
`sudo service docker start` Changes won't take effect until you restart.

The following lines didn’t work for me on an already-full docker.  But I may have failed to follow them exactly -- the existing path was to /opt/docker and I may have copied it instead (which was probably a mistake).
```bash
sudo rsync -aP /var/lib/docker/ /path/to/your/docker #move files to your new location
sudo mv /var/lib/docker /var/lib/docker.old #rename the old one while you test it
sudo service docker start
```
If it works, you can remove the old directory
`sudo rm -rf /var/lib/docker.old` 

## Dockerfiles
A Dockerfile is a set of instructions that creates a container.  It uses a simple set of instructions BUT the main pitfall is that previous instructions can override later instructions without warning.  See https://docs.docker.com/develop/develop-images/dockerfile_best-practices/
 - `FROM` creates a layer (if it’s the first one in the file, it can refer to an existing image, or can be 'FROM scratch'
`FROM scratch`
`FROM golang:1.7.3 AS builder`  
- `RUN` is used to run shell commands or to build your application with `make`.  I think Docker uses `sh` by default.
`RUN apt-get install -y curl`
- `CMD` specifies what command to run within the container.
	`CMD ["executable", "param1", "param2"…]`
- `LABEL` is specified in key=value format.  It’s just to help tracking versions.
` LABEL com.example.version="0.0.1-beta” `
- `ENV` can be used to update an environmental variable such as `$PATH`
`ENV PATH /usr/local/nginx/bin:$PATH`
- `EXPOSE` tells the container which ports to listen on
`EXPOSE 80`
- `COPY` is used for copying files into the container.  NOTE: COPY does not appear to follow links, and that seems to be a design decision rather than a bug.  So you have to hard-copy the files you want included into your directory.
`COPY requirements.txt /tmp/`
- `ADD` is also for copying, but should only be used only when its tar auto-extraction ability is needed, and
`ADD http://example.com/big.tar.xz /usr/src/things/` Avoid
in any case this command would be better done as 
`RUN curl <somefile> | tar -…blah` to avoid adding the big file to the container.

##### Download an example Docker file
`git clone https://github.com/docker/doodle.git`

##### Add Buildkit
Add this in front of a build to enable BuildKit, which has some additional features (better security, secrets, …)
`DOCKER_BUILDKIT=1 docker build .`

#### Multi-stage Builds
With multi-stage builds, you use multiple FROM statements in your Dockerfile. Each FROM instruction can use a different base, and each of them begins a new stage of the build. You can selectively copy artifacts from one stage to another, leaving behind everything you don’t want in the final image.  You can also name the stages with “AS..”, which is handy because then you can build the stages separately later. 
    `FROM golang:1.7.3 AS builder` 
    `CMD make myapp`
    ...later
    `FROM myapp` Docker will discard everything previous (build files, etc.) and start from myapp

## Images and containers
#### List images
* `docker images` Searches the directory specified in daemon.json
#### Remove images
* `docker rmi <image_id>`  You can use any of the names for an image including its tag
#### List containers
Note: Docker has renamed the `ps` command to `container ls` (two words!); all flags are identical.
* `docker container ls` List _running_ containers
* `docker container ls -a` List all containers
* `docker container ls -s` Show container sizes
* `docker container ls -a --filter "ancestor=<some_image>"` Show only the containers based on `<some_image>`.
* (See https://linuxize.com/post/how-to-list-docker-containers/ for other ls options)
* `docker ps` Show containers (older version of previous command)
* `docker ps --filter "status=exited"` Show stopped containers
#### Remove all stopped containers
* `docker rm $(docker ps -a -q)`

#### Image naming and tags
Images are named using syntax like a filepath, with a repository as the root, the image name at the end, and a tag following a colon.  So with `shykes/myapp:latest`, shykes is the repository, myapp is the image name, and :latest is the tag.  Docker tags can’t contain underscores.  You can have multiple subdirectories under a repository name.  
* Azure will use “latest” as a tag if you don’t specify one.  

## Build command
Running the `docker build` command creates a Docker image using the Dockerfile.  A Docker image is a read-only filesystem, just for your container. It provides all the files and code your container will need.  The built image is put into your machine's local Docker image registry.  Oddly, the source filename and source path are separated in the command; _the `<context>` is the sourcefile directory, not a destination_).
`docker build --no-cache -t helloapp:v2 -f dockerfiles/Dockerfile <context>` 
- By default, it looks for a file called ‘Dockerfile’ located at the root of the context.  
- `-f` Dockerfile filename _only_ (if not "Dockerfile").  The filepath goes in `<context>`! 
-  `-t` Tag the image; you can specify a different repository, but docker will not actually move the file there.  The tag in this case serves more like the address on an envelope for a subsequent ‘push’ step.  You can simultaneously assign the container to one or more repositories, as in 
`docker build -f Dockerfile -t me/myapp:1.0.1 -t me/myapp:latest .`
-  `--no-cache`  Ignore previous versions.  Can be helpful if you are trying to overwrite a version you made earlier.  But it won’t re-pull the base image; to do that you use 
-  `--pull` Re-pull the base image
- `<context>` The Dockerfile path (i.e., where to find the Dockerfile).  If you cd'd into the directory, then it is just `.`

## Run a container 
Running a container launches your software with private resources, securely isolated from the rest of your machine.    See also '_Using a GPU_', next.
- -`d` or `--detach`  Run container in background and print container ID
- `-i` or `--interactive` Run interactively, usually used with `-t`
- `-t` or `--tty` Allocate a pseudo TTY
- `-it` (with one dash) makes it run interactively
- `--rm`  (with two dashes) removes the container instance *after* it has been run, to simplify housekeeping.  It does *not* remove the image that it is created from. 
- `-p <os_port>:<container_port>` Opens a port for communication between container and local machine (e.g., `5000:8080`).  Note that this seems to enable communication between the container and the local machine, but not the outside world...but I'm not certain.
-` -v <os_directory>:<container_directory>`  Add a directory (volume) to the container.  Basically, this copies the os_directory to the container.  Very handy; put your executable script and data here.
- `<executable file>`: Added at the end of the command.  The path is relative to the container's internal root directory, `/`.  This argument is optional because you can start the container interactively and then later call a script file (as long as it's in an attached volume); see [[Docker#Debugging a containerized program|Debugging]], below, for an example.
- `/bin/bash` drop yourself into the container at inception, which is useful for testing paths, etc.
**WARNING: The container name has to come _after_ all of the named arguments** 
(but it's OK to have the script filename and `/bin/bash` afterwards)
Example:
`docker run -it jpworkspacea09a28ea.azurecr.io/tridentbase:v4 /bin/bash` 
More complex example.  Add a volume using `-v`, drop into the container,  but don't run a script.
```bash
docker run -it -v /home/egdod/jcpayne/trident_dev/runfiles:/runfiles -p 5678:5678  bea0215f6fb4 /bin/bash
```
More complex example.  Same but also run a script.  
```bash
docker run -it -v /cdata/tanzania/docker_dev:/docker_dev -p 5678:5678 c7ee859dbf144180b5d2526f88375e0c.azurecr.io/tridentbase:v8 python /docker_dev/trident_deploy.py /bin/bash
```
Exit a running container
* `Ctl-C` sometimes fails 
* `Ctl-D` Seems to also do housekeeping (clean up the container)
* `exit` At the commandline

## Copying files to/from a running container
Note:
1. You can use either the container name or the container ID as one of source/destination.
2. You cannot copy between two containers (only between one container and the host system)
3. `docker cp` is not the Linux `cp` command; it only allows two flags (`-a` for archive to preserve UID/GID information for files being copied, and `-L` for "follow links".)
```bash
docker container ls #list running containers
docker cp <src> <dest>
```


## Using a GPU in a container
**Containers will NOT automatically make use of a GPU on the host machine**; [this post](https://towardsdatascience.com/how-to-properly-use-the-gpu-within-a-docker-container-4c699c78c6d1) explains a brute-force option for adding commands to the Dockerfile, but the best approach is to add `Docker Container Kit`.  I have tested installing it on the host machine (not inside the container), and that works.  There are two options for installing it on the host machine:
Option 1: Install the 'runtime' and restart Docker:
```bash
apt-get install nvidia-container-runtime
sudo systemctl restart docker
```
Option 2. Or install the kit (which includes the runtime); then restart Docker:
```
sudo apt-get install -y nvidia-docker2
sudo systemctl restart docker
```
**In both cases, you then have to add the `--gpus all` argument to the run statement**:
```
docker run -it --rm --gpus all <yourimage> nvidia-smi #Check GPU with nvidia-smi
docker run -it --rm --gpus all <yourimage> /bin/bash etc. #An actual call
```
#### Custom Nvidia images
It is possible to base a Dockerfile on images that are provided directly by Nvidia; see [available images](https://gitlab.com/nvidia/container-images/cuda/blob/master/doc/supported-tags.md).  Note that 'base' is usually what you need; 'runtime' adds some libraries, and 'devel' adds even more; supposedly it's useful for multi-stage builds.  They include environment variables so you don't have to include `gpus all` with the run command.  The syntax is:
```bash
FROM nvidia/cuda:10.1-base-ubuntu18.04
RUN ... etc.
```
#### docker create/docker start
The `docker create` command creates a writeable container layer over the specified image and prepares it for running the specified command. The container ID is then printed to STDOUT. This is similar to `docker run -d` except the container is never started. You can then use the `docker start <container_id>` command to start the container at any point.


## Debugging a containerized program
1. Make a directory with the script you want to run in it
2. Put the resources you'll need in a directory, including script and input files
	- `cp /cdata/tanzania/temp/tiled_images/RR17-CFA-R-2014-11-04_AM_0073[4][3].jpg /cdata/tanzania/docker_dev/`
	- `cp /home/user/runscript.py /cdata/tanzania/docker_dev/`
3. Add that directory to the Docker container in the run step, using `-v source_dir:dest_dir` 
4. Run the container interactively
	- `docker run -it -v /cdata/tanzania/docker_dev:/docker_dev c7ee859dbf144180b5d2526f88375e0c.azurecr.io/tridentbase:v11 /bin/bash`
5. Once inside the container, run the script from that directory:   
	- `python docker_dev/trident_deploy.py`

#### Useful commands for debugging:
1. `docker logs <container_id>`
Gives the full STDOUT and STDERR from the command that was run initially in your container.
2. `docker stats <container_id>`
If you just need to keep an eye on the metrics of your container to work out what’s gone wrong, docker stats can help: it’ll give you a live stream of resource usage, so you can see just how much memory you’ve leaked so far.
3. `docker cp <container_id>:/path/to/useful/file /local-path`
Often just getting hold of more log files is enough to sort you out. Copies any file from any container back out onto your local machine, so you can examine it in depth.  See [[Docker#Copying files to/from a running container]], above.
4. `docker exec -it <container_id> /bin/bash`
Run the container interactively (if it’s crashed, you can restart it with `docker start <container_id>`).
5. `docker commit <container_id> my-broken-container &&
docker run -it my-broken-container /bin/bash`
Can’t start your container at all? If you’ve got a initial command or entrypoint that immediately crashes, Docker will immediately shut it back down for you. This can make your container unstartable, so you can’t shell in any more, which really gets in the way.  Fortunately, there’s a workaround: save the current state of the shut-down container as a new image, and start that with a different command to avoid your existing failures.  Have a failing entrypoint instead? There’s an entrypoint override command-line flag too.

## Other details
### Communicating with a local container via http
If you’re using Docker Toolbox, then any port you publish with (`docker run -p`) will be published on the Toolbox VM’s private IP address. Command (`docker-machine ip`) (perhaps only for Windows and Mac???) will tell you the correct IP address to use. It is frequently 192.168.99.100. Then you can *browse to (192.168.99.100:8080) instead of (localhost:8080)*.

### Make an image public on Docker Hub
Once you're ready to share your container with the world, push the image that describes it to Docker Hub.
* `docker login`
* `docker push urukhai/myimage`

